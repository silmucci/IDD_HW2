OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender

OneTrans: Unified Feature Interaction and Sequence Modeling
with One Transformer in Industrial Recommender
Zhaoqi Zhangâˆ—
Nanyang Technological University
ByteDance
Singapore, Singapore
zhaoqi.zhang@bytedance.comHaolei Peiâˆ—
ByteDance
Singapore, Singapore
haolei.pei@bytedance.comJun Guoâˆ—
ByteDance
Singapore, Singapore
jun.guo@bytedance.com
Tianyu Wang
ByteDance
Singapore, Singapore
tianyu.wang01@bytedance.comYufei Feng
ByteDance
Hangzhou, China
fengyihui@bytedance.comHui Sun
ByteDance
Hangzhou, China
sunhui.sunh@bytedance.com
Shaowei Liuâ€ 
ByteDance
Singapore, Singapore
liushaowei.nphard@bytedance.comAixin Sunâ€ 
Nanyang Technological University
Singapore, Singapore
axsun@ntu.edu.sg
Abstract
In recommendation systems, scaling up feature-interaction mod-
ules (e.g., Wukong, RankMixer) or user-behavior sequence modules
(e.g., LONGER) has achieved notable success. However, these ef-
forts typically proceed on separate tracks, which not only hinders
bidirectional information exchange but also prevents unified op-
timization and scaling. In this paper, we proposeOneTrans, a
unified Transformer backbone that simultaneously performs user-
behavior sequence modeling and feature interaction.OneTrans
employs a unified tokenizer to convert both sequential and non-
sequential attributes into a single token sequence. The stacked
OneTransblocks share parameters across similar sequential to-
kens while assigning token-specific parameters to non-sequential
tokens. Through causal attention and cross-request KV caching,
OneTransenables precomputation and caching of intermediate
representations, significantly reducing computational costs during
both training and inference. Experimental results on industrial-
scale datasets demonstrate thatOneTransscales efficiently with
increasing parameters, consistently outperforms strong baselines,
and yields a 5.68% lift in per-user GMV in online A/B tests.
CCS Concepts
â€¢Information systemsâ†’Recommender systems.
âˆ—These authors contributed equally.
â€ Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
Conference acronym â€™XX, Woodstock, NY
Â©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-XXXX-X/18/06Keywords
Recommender System, Ranking Model, Scaling Laws
ACM Reference Format:
Zhaoqi Zhang, Haolei Pei, Jun Guo, Tianyu Wang, Yufei Feng, Hui Sun,
Shaowei Liu, and Aixin Sun. 2025. OneTrans: Unified Feature Interaction
and Sequence Modeling with One Transformer in Industrial Recommender.
InProceedings of Make sure to enter the correct conference title from your
rights confirmation email (Conference acronym â€™XX).ACM, New York, NY,
USA, 9 pages.
1 Introduction
Recommendation systems (RecSys) play a fundamental role in vari-
ous information services, such as e-commerce [ 9,31], streaming me-
dia [2,19,26] and social networks [ 28]. Industrial RecSys generally
adopt a cascaded ranking architecture [ 6,16,21]. First, a recall stage
selects hundreds of candidates from billion-scale corpora [ 13,32].
Then, a ranking stage â€” often with coarse- and fine-ranking â€”
scores each candidate and returns the top- ğ‘˜items [ 11,25,26,28,33].
We focus on the ranking stage in this paper. For ranking, main-
stream approaches iterate on two separate modules: (a)sequence
modeling, which encodes user multi-behavior sequences into candidate-
aware representations using local attention or Transformer en-
coders [ 1,14,23,31], and (b)feature interaction, which learns high-
order crosses among non-sequential features (e.g., user profile,
item profile, and context) via factorization or explicit cross net-
works, or attention over feature groups [ 11,12,25,33]. As shown
in Fig. 1(a), these approaches typically encode user behaviors into
acompressedsequence representation, then concatenate it with
non-sequential features and apply a feature-interaction module
to learn higher-order interaction; we refer to this design as the
encode-then-interactionpipeline.
The success of large language models (LLMs) demonstrates that
scaling model size (e.g., parameter size, training data) yields pre-
dictable gains in performance [ 15], inspiring similar investigations
within RecSys [ 1,28,33]. For feature interaction, Wukong [ 28]
stacks Factorization Machine blocks with linear compression toarXiv:2510.26104v1  [cs.IR]  30 Oct 2025
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Zhang et al.
OneTransStackNon-Seq FeaturesSequential FeaturesMulti-task TowerFeature Interaction Block
Non-Seq FeaturesSequential FeaturesMulti-task Tower
Sequence Modeling BlockTokenizer(a)Conventional Approach(b)OneTransCompressedSeq
Figure 1: Architectural comparison. (a) Conventionalencode-
then-interactionpipeline encodes sequential features and
merges non-sequential features before a post-hoc feature
interaction block. (b)OneTransperforms joint modeling of
both sequential and non-sequential features within a single
OneTrans(Transformer-style) stack.
capture high-order feature interactions and establishes scaling laws,
while RankMixer [ 33] achieves favorable scaling through hardware-
friendly token-mixing with token-specific feed-forward networks
(FFNs). For sequence modeling, LONGER[ 1] applies causal Trans-
formers to long user histories and shows that scaling depth and
width yields monotonic improvements. Although effective in prac-
tice, separating sequence modeling and feature interaction as in-
dependent modules introduces two major limitations. First, the
encode-then-interaction pipeline restricts bidirectional information
flow, limiting how static/context features shape sequence represen-
tations [ 27]. Second, module separation fragments execution and
increases latency, whereas a single Transformer-style backbone
can reuse LLM optimizations e.g., KV caching, memory-efficient
attention, and mixed precision, for more effective scaling [11].
In this paper, we proposeOneTrans, an innovative architec-
tural paradigm with a unified Transformer backbone that jointly
performs user-behavior sequence modeling and feature interaction.
As shown in Fig. 1(b),OneTransenables bidirectional informa-
tion exchange within the unified backbone. It employs a unified
tokenizer that converts bothsequentialfeatures (diverse behav-
ior sequences) andnon-sequentialfeatures (static user/item and
contextual features) into a single token sequence, which is then
processed by a pyramid of stackedOneTransblocks, a Transformer
variant tailored for industrial RecSys. To accommodate the diverse
token sources in RecSys, unlike the text-only tokens in LLMs, each
OneTransblock adopts amixedparameterization similar to Hi-
Former [ 11]. Specifically, allsequentialtokens (from sequential
features) share a single set of Q/K/V and FFN weights, while each
non-sequentialtoken (from non-sequential features) receivestoken-
specificparameters to preserve its distinct semantics.
Unlike conventional encode-then-interaction frameworks,One-
Transeliminates the architectural barrier betweensequentialand
non-sequentialfeatures through a unified causal Transformer back-
bone. This formulation brings RecSys scaling in line with LLM
practices: theentiremodel can be scaled by adjusting backbone
depth and width, while seamlessly inheriting mature LLM optimiza-
tions, such as FlashAttention [ 7], and mixed precision training [ 17].Particularly, cross-candidate and cross-request KV caching [ 1] re-
duces the time complexity from ğ‘‚(ğ¶) toğ‘‚(1)for sessions with ğ¶
candidates, making large-scaleOneTransdeployment feasible.
In summary, our main contributions are fourfold:(1) Unified
framework.We presentOneTrans, a single Transformer back-
bone for ranking, equipped with aunified tokenizerthat encodes
sequential and non-sequential features into one token sequence, and
aunified Transformer blockthat jointly performs sequence modeling
and feature interaction.(2) Customization for recommenders.
To bridge the gap between LLMs and RecSys tasks,OneTrans
introduces amixed parameterizationthat allocates token-specific
parameters to diverse non-sequential tokens while sharing param-
eters for all sequential tokens.(3) Efficient training and serving.
We improve efficiency with apyramid strategythat progressively
prunes sequential tokens and across-request KV Cachingthat reuses
user-side computations across candidates. In addition, we adopt
LLM optimizations such as FlashAttention, mixed-precision train-
ing, and half-precision inference to further reduce memory and
compute.(4) Scaling and deployment.OneTransdemonstrates
near log-linear performance gains with increased model size, pro-
viding evidence of a scaling law in real production data. When
deployed online, it achieves statistically significant lifts on business
KPIs while maintaining production-grade latency.
2 Related Work
Early RecSys like DIN [ 31] and its session-aware variants (DSIN) [ 9]
use local attention to learn candidate-conditioned summaries of
user histories, but compress behaviors into fixed-length vectors
per candidate, limiting long-range dependency modeling [ 30]. Self-
attentive methods like SASRec [ 14], BERT4Rec [ 23], and BST [ 4]
eliminate this bottleneck by letting each position attend over the
full history and improve sample efficiency with bidirectional mask-
ing. Recently, as scaling laws [ 15] in RecSys are increasingly ex-
plored, LONGER [ 1] pushes sequence modeling toward industrial
scales by targeting ultra-long behavioral histories with efficient
attention and serving-friendly designs. However, in mainstream
pipelines these sequence encoders typically remainseparatefrom
the feature-interaction stack, leading to late fusion rather than joint
optimization with static contextual features [27].
On the feature-interaction side, early RecSys rely on manually en-
gineered cross-features or automatic multiplicative interaction lay-
ers. Classical models such as Wide&Deep [ 5], FM/DeepFM [ 3,12],
and DCN/DCNv2 [ 24,25] provide efficient low-order or bounded-
degree interactions. However, as recent scaling studies observe [ 28],
once the model stacks enough cross layers, adding more stops help-
ing: model quality plateaus instead of continuing to improve. To
overcome the rigidity of preset cross forms, attention-based ap-
proaches automatically learn high-order interactions. AutoInt [ 22]
learns arbitrary-order relations, and HiFormer [ 11] introduces group-
specific projections to better capture heterogeneous, asymmet-
ric interactions. With scaling up increasingly applied tofeature-
interactionmodules, large-scale systems such as Wukong [ 28] demon-
strate predictable gains by stacking FM-style interaction blocks with
linear compression, while RankMixer [ 33] achieves favorable scal-
ing via parallel token mixing and sparse MoE under strict latency
budgets. However, these interaction modules typically adhere to the
OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
SequentialTokenizerNon-SeqTokenizerSEPSEPNStokensStokensOneTransBlockxNOneTransBlockxNOneTransBlockxNTaskTowerOneTransPyramidStack
NS/StokensRMSNorm
RMSNormMix FFN
MixCausalAttentionOutput
xN++
PosEmbFFNFFNFFNMixFFN
MixCausalAttention
QKVQKVQKVMulti Head Attention
(a) OneTrans Framework(b) OneTransBlock(c) Mix Parameterization
Figure 2: System Architecture. (a)OneTransoverview. Sequential (S, blue) and non-sequential (NS, orange) features are
tokenized separately. After inserting [SEP] between user behavior sequences, the unified token sequence is fed into stacked
OneTransPyramid Blocksthat progressively shrink the token length until it matches the number of NS tokens. (b)OneTrans
Block: a causal pre-norm Transformer Block with RMSNorm,Mixed Causal AttentionandMixed FFN. (c) â€œMixedâ€ = mixed
parameterization: S tokens share one set of QKV/FFN weights, while each NS token receives its own token-specific QKV/FFN.
interactionparadigm, which pushes interactions to a separate stage
and blocks unified optimization with user sequence modeling [ 27].
To date, progress in RecSys has largely advanced along two
independent tracks: sequence modeling and feature interaction.
InterFormer [ 27] attempts to bridge this gap through a summary-
based bidirectional cross architecture that enables mutual signal
exchange between the two components. However, it still maintains
them as separate modules, and the cross architecture introduces
both architectural complexity and fragmented execution. Without
a unified backbone for joint modeling and optimization, scaling the
system as an integrated whole remains challenging.
3 Methodology
Before detailing our method, we briefly describe the task setting.
In a cascaded industrial RecSys, each time the recall stage returns a
candidate set (typically hundreds of candidate items) for a user ğ‘¢.
The ranking model then predicts a score to each candidate itemğ‘–:
Ë†ğ‘¦ğ‘¢,ğ‘–=ğ‘“ ğ‘–NS,S;Î˜(1)
whereNS is a set of non-sequential features derived from the
user, the candidate item, and the context; Sis a set of historical
behavior sequences from the user; and Î˜are trainable parameters.
Common task predictions include the click-through rate (CTR) and
the post-click conversion rate (CVR).
CTRğ‘¢,ğ‘–=ğ‘ƒ click=1NS,S;Î˜,
CVRğ‘¢,ğ‘–=ğ‘ƒ conv=1click=1,NS,S;Î˜.(2)
3.1OneTransFramework Overview
As illustrated in Fig. 2(a),OneTransemploys aunified tokenizerthat
maps sequential features Sto S-tokens, and non-sequential featuresNS to NS-tokens. Apyramid-stacked Transformerthen consumes
the unified token sequence jointly within a single computation
graph. We denote the initial token sequence as
X(0)=
S-tokens;NS-tokens
âˆˆR(ğ¿S+ğ¿NS)Ã—ğ‘‘.(3)
This token sequence is constructed by concatenating ğ¿Snumber
of S-tokens and ğ¿NSnumber of NS-tokens, with all tokens having
dimensionality ğ‘‘. Note that, the S-tokens contain learnable [SEP]
tokens inserted to delimit boundaries between different kind of
user-behavior sequences. As shown in Fig. 2(b), eachOneTrans
block progressively refines the token states through:
Z(ğ‘›)=MixedMHA
Norm X(ğ‘›âˆ’1)
+X(ğ‘›âˆ’1),(4)
X(ğ‘›)=MixedFFN
Norm Z(ğ‘›)
+Z(ğ‘›).(5)
Here, MixedMHA (Mixed Multi-Head Attention) and MixedFFN
(Mixed Feed-Forward Network) adopt a mixed parameterization
strategy (see Fig. 2(c)) sharing weights across sequential tokens,
while assigning separate parameters to non-sequential tokens in
both the attention and feed-forward layers.
A unified causal mask enforces autoregressive constraints, re-
stricting each position to attend only to preceding tokens. Specifi-
cally, NS-tokens are permitted to attend over the entire history of
S-tokens, thereby enabling comprehensive cross-token interaction.
By stacking such blocks with pyramid-style tail truncation applied
to S-tokens, the model progressively distills compact high-order
information into the NS-tokens. The final token states are then
passed to task-specific heads for prediction.
By unifying non-sequential and sequential features into a uni-
fied token sequence and modeling them with a causal Transformer,
OneTransdeparts from the conventionalencode-then-interaction
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Zhang et al.
pipeline. This unified design naturally enables (i)intra-sequence
interactions within each behavior sequence, (ii)cross-sequencein-
teractions across multiple sequences, (iii)multi-source featureinter-
actions among item, user, and contextual features, and (iv)sequence-
featureinteractions,all within a single Transformer stack.
The unified formulation enables us to seamlessly inherit mature
LLM engineering optimizations, including KV caching and memory-
efficient attention, thereby substantially reducing inference latency.
We argue this unified formulation is well suited to tackling multi-
sequence and cross-domain recommendation challenges in a single,
and scalable architecture. Next, we detail the design.
3.2 Features and Tokenization
To construct the initial token sequenceX(0),OneTransfirst applies
a feature preprocessing pipeline that maps all raw feature inputs
into embedding vectors. These embeddings are then partitioned into
(i) a multi-behaviorsequentialsubset and (ii) anon-sequentialsubset
representing user, item, or context features. Separate tokenizers are
applied to each subset.
3.2.1 Non-Sequential Tokenization.Non-sequential features NS
include both numerical inputs (e.g., price, CTR) and categorical in-
puts (e.g., user ID, item category). All features are either bucketized
or one-hot encoded and then embedded. Since industrial systems
typically involve hundreds of features with varying importance,
there are two options for controlling the number of non-sequential
tokens, denoted byğ¿ ğ‘ğ‘†:
Group-wise Tokenizer(aligned with RankMixer [ 33]). Features
are manually partitioned into semantic groups {g1,..., gğ¿ğ‘ğ‘†}. Each
group is concatenated and passed through a group-specific MLP:
NS-tokens=
MLP 1(concat(g 1)),...,MLP ğ¿ğ‘ğ‘†(concat(g ğ¿ğ‘ğ‘†))
.
(6)
Auto-Split Tokenizer. Alternatively, all features are concatenated
and projected once by a single MLP, then split:
NS-tokens=split
MLP(concat(NS)), ğ¿ ğ‘ğ‘†
.(7)
Auto-Split Tokenizer reduces kernel launch overhead compared
with Group-wise approach, by using a single dense projection. We
will evaluate both choices through experiments.
Ultimately, non-sequential tokenization yields ğ¿ğ‘ğ‘†number of
non-sequential tokens, each of dimensionalityğ‘‘.
3.2.2 Sequential Tokenization.OneTransaccepts multi-behavior
sequences as
S={S 1,...,Sğ‘›},Sğ‘–=
eğ‘–1,...,eğ‘–ğ¿ğ‘–
.(8)
Each sequenceS ğ‘–consists ofğ¿ğ‘–number of event embeddingse,
which is constructed by concatenating the item ID with its corre-
sponding side information like item category and price.
Multi-behavior sequences can vary in their raw dimensionality.
Hence, for each sequenceS ğ‘–, we use one shared projection MLPğ‘–to
convert its all evente ğ‘–ğ‘—as a common dimensionalityğ‘‘:
ËœSğ‘–=
MLPğ‘–(eğ‘–1),...,MLP ğ‘–(eğ‘–ğ¿ğ‘–)
âˆˆRğ¿ğ‘–Ã—ğ‘‘.(9)
Aligned sequences ËœSğ‘–are merged into a single token sequence by
one of two rules: 1)Timestamp-aware: interleave all events by time,
with sequence-type indicators; 2)Timestamp-agnostic: concatenatesequences by event impact, e.g., purchase â†’add-to-cartâ†’click,
inserting learnable [SEP] tokens between sequences. In the latter,
behaviors with higher user intent are placed earlier in the sequence.
Ablation results indicate that, when timestamps are available, the
timestamp-aware rule outperforms the impact-ordered alternative.
Formally, we have:
S-Tokens=Merge ËœS1,..., ËœSğ‘›âˆˆRğ¿ğ‘†Ã—ğ‘‘, ğ¿ğ‘†=ğ‘›âˆ‘ï¸
ğ‘–=1ğ¿ğ‘–+ğ¿SEP.(10)
3.3OneTransBlock
As shown in Fig. 2(b), eachOneTransblock is a pre-norm causal
Transformer applied to anormalizedtoken sequence: ğ¿ğ‘†sequential
S-tokens, followed by ğ¿ğ‘ğ‘†non-sequentialNS-tokens. Inspired by
the findings on heterogeneous feature groups [ 11], we make a light-
weight modification to Transformer to allow a mixed parameter
scheme, see Fig. 2(c). Specifically, homogeneous S-tokens share
one set of parameters. The NS-tokens, being heterogeneous across
sources/semantics, receive token-specific parameters.
Unlike LLM inputs, the token sequence in RecSys combines se-
quential S-tokens with diverse NS-tokens whose value ranges and
statistics differ substantially. Post-norm setups can cause atten-
tion collapse and training instability due to these discrepancies. To
prevent this, we apply RMSNorm [ 29] as pre-norm toalltokens,
aligning scales across token types and stabilizing optimization.
3.3.1 Mixed (shared/token-specific) Causal Attention.OneTrans
adopts a standard multi-head attention (MHA) with a causal at-
tention mask; the only change is how Q/K/V are parameterized.
Letxğ‘–âˆˆRğ‘‘be theğ‘–-th token. To compute Q/K/V, we use ashared
projection for S-tokens (ğ‘–â‰¤ğ¿ ğ‘†) andğ¿ğ‘ğ‘†token-specificprojections
for NS-tokens (ğ‘–>ğ¿ ğ‘†):
 qğ‘–,kğ‘–,vğ‘–= Wğ‘„
ğ‘–xğ‘–,Wğ¾
ğ‘–xğ‘–,Wğ‘‰
ğ‘–xğ‘–,(11)
whereWÎ¨
ğ‘–(Î¨âˆˆ{ğ‘„,ğ¾,ğ‘‰} ) follows a mixed parameterization scheme:
WÎ¨
ğ‘–=ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³WÎ¨
S, ğ‘–â‰¤ğ¿ ğ‘†(shared for S-tokens),
WÎ¨
NS,ğ‘–, ğ‘–>ğ¿ğ‘†(token-specific for NS-tokens).(12)
Attention uses a standardcausalmask, with NS-tokens placed
afterS-tokens. This induces: (1)S-side.Each S-token attends only
to earlierğ‘†positions. Fortimestamp-awaresequences, every event
conditions on its history; fortimestamp-agnosticsequences (or-
dered by intent, e.g., purchase â†’add-to-cartâ†’click/impression),
causal masking lets high-intent signals inform and filter later low-
intent behaviors. (2)NS-side.Every NS-token attends to theentire
ğ‘†history, effectively a target-attention aggregation of sequence
evidence, and toprecedingNS-tokens, increasing token-level inter-
action diversity. (3)Pyramid support.On both S and NS sides,
causal masking progressively concentrates information toward later
positions, naturally supporting the pyramid schedule that prunes
tokens layer by layer, to be detailed shortly.
3.3.2 Mixed (shared/token-specific) FFN.Similarly, the feed-forward
network follows the same parameterization strategy: token-specific
FFNs for NS-tokens, and a shared FFN for S-tokens,
MixedFFN(x ğ‘–)=W2
ğ‘–ğœ™(W1
ğ‘–xğ‘–).(13)
OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
HereW1
ğ‘–andW2
ğ‘–follow the mixed parameterization of Eqn. (12),
i.e., shared forğ‘–â‰¤ğ¿ ğ‘†and token-specific forğ‘–>ğ¿ ğ‘†.
In summary, relative to a standard causal Transformer,One-
Transchanges only theparameterization: NS-tokens usetoken-
specificQKV and FFN; S-tokenssharea single set of parameters. A
single causal mask ties the sequence together, allowing NS-tokens
to aggregate the entire behavior history while preserving efficient,
Transformer-style computation.
3.4 Pyramid Stack
As noted in Section 3.3, causal masking concentrates information
toward later positions. Exploiting this recency structure, we adopt
apyramidschedule: at eachOneTransblock layer, only a subset of
the most recent S-tokens issue queries, while keys/values are still
computed over the full sequence; the query set shrinks with depth.
LetX ={xğ‘–}ğ¿
ğ‘–=1be the input token list and Q={ğ¿âˆ’ğ¿â€²+1,...,ğ¿}
denote a tail index set with ğ¿â€²â‰¤ğ¿. Following Eqn. 12, we modify
queriesasğ‘–âˆˆQ:
qğ‘–=Wğ‘„
ğ‘–xğ‘–, ğ‘–âˆˆQ,(14)
while keys and values are computed as usual over the full sequence
{1,...,ğ¿} . After attention, only outputs for ğ‘–âˆˆ Q are retained,
reducing the token length to ğ¿â€²and forming a pyramidal hierarchy
across layers.
This design yields two benefits: (i)Progressive distillation: long be-
havioral histories are funneled into a small tail of queries, focusing
capacity on the most informative events and consolidating infor-
mation into the NS-tokens; and (ii)Compute efficiency: attention
cost becomes ğ‘‚ ğ¿ğ¿â€²ğ‘‘and FFN scales linearly with ğ¿â€². Shrinking
the query set directly reduces FLOPs and activation memory.
3.5 Training and Deployment Optimization
3.5.1 Cross Request KV Caching.In industrial RecSys, samples
from the same request are processed contiguously both during
training and serving: their S-tokens remain identical across candi-
dates, while NS-tokens vary per candidate item. Leveraging this
structure, we integrate the widely adopted KV Caching [ 1] into
OneTrans, yielding a unified two-stage paradigm.
Stage I (S-side, once per request).Process all S-tokens with causal
masking and cache their key/value pairs and attention outputs. This
stage executesonceper request.
Stage II (NS-side, per candidate).For each candidate, compute
its NS-tokens and perform cross-attention against the cached S-
side keys/values, followed by token-specific FFN layers. Specially,
candidate-specific sequences (e.g., SIM [ 20]) are pre-aggregated
into NS-tokens via pooling, as they cannot reuse the shared S-side
cache.
The KV Caching amortizes S-side computation across candidates,
keeping per-candidate work lightweight and eliminating redundant
computations for substantial throughput gains.
Since user behavioral sequences are append-only, we extend KV
Cachingacross requests: each new request reuses the previous cache
and computes only the incremental keys/values for newly added
behaviors. This reduces per-request sequence computation from
ğ‘‚(ğ¿) toğ‘‚(Î”ğ¿) , where Î”ğ¿is the number of new behaviors since
the last request.Table 1: Dataset overview forOneTransexperiments.
Metric Value
# Impressions (samples) 29.1B
# Users (unique) 27.9M
# Items (unique) 10.2M
Daily impressions (meanÂ±std) 118.2MÂ±14.3M
Daily active users (meanÂ±std) 2.3MÂ±0.3M
3.5.2 Unified LLM Optimizations.We employ FlashAttention-2 [ 8]
to reduce attention I/O and the quadratic activation footprint of
vanilla attention via tiling and kernel fusion, yielding lower mem-
ory usage and higher throughput in both training and inference.
To further ease memory pressure, we use mixed-precision train-
ing (BF16/FP16) [ 18] together with activation recomputation [ 10],
which discards selected forward activations and recomputes them
during backpropagation. This combination trades modest extra
compute for substantial memory savings, enabling larger batches
and deeper models without architectural changes.
4 Experiments
Through both offline evaluations and online tests, we aim to an-
swer the following Research Questions (RQs):RQ1: Unified stack
vs. encodeâ€“thenâ€“interaction.Does thesingle Transformer stack
yield consistent performance gains under the comparable compute?
RQ2: Which design choices matter?We conduct ablations on
theinput layer(e.g., tokenizer, sequence fusion) and theOneTrans
block(e.g., parameter sharing, attention type, pyramid stacking) to
evaluate the importance of different design choices for performance
and efficiency.RQ3: Systems efficiency.Do pyramid stacking,
cross-request KV Caching, FlashAttention-2, and mixed precision
with recomputation reduce FLOPs/memory and latency under the
sameOneTransgraph?RQ4: Scaling law.As we scale length
(token sequence length), width ( ğ‘‘model ), depth (number of layers),
do loss/performance exhibit the expectedlog-lineartrend?RQ5:
Online A/B Tests.Does deployingOneTransonline yield statisti-
cally significant lifts in key business metrics (e.g., order/u, GMV/u)
under production latency constraints?
4.1 Experimental Setup
4.1.1 Dataset.For offline evaluation, we evaluateOneTransin a
large-scale industrial ranking scenario using production logs under
strict privacy compliance (all personally identifiable information
is anonymized and hashed). Data are split chronologically, with
all features snapshotted at impression time to prevent temporal
leakage and ensure online-offline consistency. Labels (e.g., clicks
and orders) are aggregated within fixed windows aligned with
production settings. Table 1 summarizes the dataset statistics.
4.1.2 Tasks and Metrics.We evaluate two binary ranking tasks
as defined in Eqn. (2): CTR and CVR. Performance is measured by
AUC and UAUC (impression-weighted user-level AUC).
Next-batch evaluation.Data are processed chronologically.
For each mini-batch, we (i) log predictions in eval mode, then (ii)
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Zhang et al.
Table 2: Offline effectiveness (CTR/CVR) and efficiency; higher AUC/UAUC is better. Efficiency figures are collected from
prior works, and dashes indicate unavailable. * indicates models deployed in our production lineage in chronological order:
DCNv2+DINâ†’RankMixer+DINâ†’RankMixer+Transformerâ†’OneTrans Sâ†’OneTrans L(default)
TypeModelCTR CVR (order) Efficiency
AUCâ†‘UAUCâ†‘ AUCâ†‘UAUCâ†‘ Params (M) TFLOPs
(1) Base modelDCNv2 + DIN (base)* 0.79623 0.71927 0.90361 0.71955 10 0.06
(2) Feature-interactionWukong + DIN +0.08% +0.11% +0.14% +0.11% 28 0.54
HiFormer + DIN +0.11% +0.18% +0.23% -0.20% 108 1.35
RankMixer + DIN* +0.27% +0.36% +0.43% +0.19% 107 1.31
(3) Sequence-modelingRankMixer + StackDIN +0.40% +0.37% +0.63% -1.28% 108 1.43
RankMixer + LONGER +0.49% +0.59% +0.47% +0.44% 109 1.87
RankMixer + Transformer* +0.57% +0.90% +0.52% +0.75% 109 2.51
(4) Unified frameworkOneTrans S* +1.13% +1.77% +0.90% +1.66% 91 2.64
OneTrans L(default)* +1.53% +2.79% +1.14% +3.23% 330 8.62
train on the same batch. AUC and UAUC are computed daily from
each dayâ€™s predictions and finally macro-averaged across days.
Efficiency metrics.We reportParams(model parameters ex-
cluding sparse embeddings) andTFLOPs(training compute in TFLOPs
at batch size 2048).
4.1.3 Baselines.We construct industry-standard model combi-
nations as baselines using the same features and matched com-
pute budgets. Under theencode-then-interactionparadigm, we start
from the widely-used production baselineDCNv2+DIN[ 25,31]
and progressively strengthen thefeature-interactionmodule:
DCNv2â†’Wukong [ 28]â†’HiFormer [ 11]â†’RankMixer [ 33]. With
RankMixer fixed, we then vary thesequence-modelingmodule:
StackDINâ†’Transformer [4]â†’LONGER [1].
4.1.4 Hyperparameter Settings.We report two settings:OneTrans-
Suses6stackedOneTransblocks width ğ‘‘=256, andğ»=4heads,
targetingâ‰ˆ100M parameters.OneTrans-Lscales to8layers with
widthğ‘‘=384(stillğ»=4). Inputs are processed through a unified to-
kenizer: multi-behavior sequences are fused in atimestamp-aware
manner, while non-sequential features are tokenized viaAuto-Split.
The pyramid schedule linearly reduces tokens from 1190 to 12.
Optimization and infrastructure.We use a dual-optimizer
strategy without weight decay: sparse embeddings are optimized
with Adagrad ( ğ›½1=0.1,ğ›½2=1.0), and dense parameters with RM-
SPropV2 (lr =0.005, momentum =0.99999). The per-GPU batch size
is set to 2048 during training, with gradient clipping thresholds
of 90 for dense layers and 120 for sparse layers to ensure stable
optimization. For online inference, we adopt a smaller batch size
of 100 per GPU to balance throughput and latency. Training uses
data-parallel all-reduce on 16 H100 GPUs.
4.2 RQ1: Performance Evaluation
We anchor our comparison on DCNv2+DIN, the pre-scaling pro-
duction baseline in our scenario (Table 2). Under theencode-then-
interactionparadigm, scaling either component independently is
beneficial: upgrading thefeature interactionmodule (DCNv2 â†’
Wukongâ†’HiFormerâ†’RankMixer) or thesequence modelingmodule (StackDIN â†’Transformerâ†’LONGER) yields consistent
gains in CTR AUC/UAUC and CVR AUC. In our system, improve-
ments above+0.1%in these metrics are considered meaningful,
while gains above +0.3%typically correspond to statistically sig-
nificant effects in online A/B tests. However, CVR UAUC is treated
cautiously due to smaller per-user sample sizes and higher volatility.
Moving to a unified design,OneTrans Ssurpasses the baseline
by+1.13%/+1.77%(CTR AUC/UAUC) and +0.90%/+1.66%(CVR
AUC/UAUC). At a comparable parameter scale, it also outperforms
RankMixer+Transformer with similar training FLOPs (2.64T vs.
2.51T), demonstrating the benefits of unified modeling. Scaling fur-
ther,OneTrans Ldelivers the best overall improvement of +1.53%
/+2.79%(CTR AUC/UAUC) and +1.14%/+3.23%(CVR AUC/UAUC),
showing a predictable quality performance as model capacity grows.
In summary, unifying sequence modeling and feature interaction
in a single Transformer yields more reliable and compute-efficient
improvements than scaling either component independently.
4.3 RQ2: Design Choices via Ablation Study
We perform an ablation study of the proposedOneTrans Smodel
to quantify the contribution of key design choices. The complete
results are summarized in Table 3. We evaluate the following vari-
ants:Input variants:i) Replacing theAuto-Split Tokenizerwith a
Group-wise Tokenizer(Row 1); ii) Using a timestamp-agnosticfusion
strategy instead of the timestamp-awaresequence fusion (Row 2);
iii) Removing [SEP] tokens in the timestamp-awaresequence fu-
sion (Row 3);OneTrans block variants:i) Sharing a single set of
Q/K/V and FFN parameters acrossalltokens, instead of assigning
separate parameters to NS-tokens (Row 4); ii) Replacing causal at-
tention with full attention (Row 5); iii) Disabling the pyramid stack
by keeping the full token sequence atalllayers (Row 6).
In summary, the ablations show that 1)Auto-Split Tokenizer
provides a clear advantage over manually grouping non-sequential
features into tokens, indicating that allowing the model to automat-
ically build non-sequential tokens is more effective than relying on
human-defined feature grouping; 2)Timestamp-aware fusion
beats intent-based ordering when timestamps exist, suggesting that
OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
Table 3: Impact of the choices of input design andOneTransblock design, using the OneTrans Smodel as the reference.
TypeVariantCTR CVR (order) Efficiency
AUCâ†‘UAUCâ†‘ AUCâ†‘UAUCâ†‘ Params (M) TFLOPs
InputGroup-wise Tokenzier -0.10% -0.30% -0.12% -0.10% 78 2.35
Timestamp-agnostic Fusion -0.09% -0.22% -0.20% -0.21% 91 2.64
Timestamp-agnostic Fusion w/o Sep Tokens -0.13% -0.32% -0.29% -0.33% 91 2.62
OneTrans BlockShared parameters -0.15% -0.29% -0.14% -0.29% 24 2.64
Full attention +0.00% +0.01% -0.03% +0.06% 91 2.64
w/o pyramid stack -0.05% +0.06% -0.04% -0.42% 92 8.08
Table 4: Key efficiency comparison between OneTrans Land
the DCNv2+DIN baseline.
Metric DCNv2+DIN OneTrans L
TFLOPs 0.06 8.62
Params (M) 10 330
MFU 13.4 30.8
Inference Latency (p99, ms) 13.6 13.2
Training Memory (GB) 20 32
Inference Memory (GB) 1.8 0.8
temporal ordering should be prioritized over event impact; 3) Un-
der timestamp-agnosticfusion, learnable [SEP] tokenshelp the
model separate sequences; 4) Assigningtoken-specific parame-
tersto NS-tokens yields clear gains over sharing one set across all
tokens, demonstrating that modeling non-sequential features with
individualized projections enables better feature discrimination; 5)
Causal and full attentionachieve similar results, indicating that
allowing tokens to attend to future positions is not crucial in this
setting. Notably, we emphasize that full attention prohibits the use
of standard optimizations such as KV caching; 6) Retaining the full
token list at every layer provides no benefit:OneTranseffectively
summarizes information into a small tail of tokens, so thepyramid
designcan safely prune queries to save computation.
4.4 RQ3: Systems Efficiency
To quantify the optimizations in Section 3.5, we ablate them on
an unoptimizedOneTrans Sbaseline and report training/inference
metrics in Table 5. The unoptimized OneTrans Sruns at 407 ms train-
ing runtime with 53.13 GB peak training memory, and 54.00 ms p99
inference latency with 1.70 GB inference memory, wherep99de-
notes the 99th-percentile (tail) latencyâ€”a standard SLO metric for
high-availability online services. These differences reflect distinct
operating conditions: offline training uses large per-device batches,
while online inference distributes micro-batches across machines
for stability. As shown in the table, 1)Pyramid stackyields sub-
stantial savings (âˆ’28.7%training time,âˆ’42.6%training memory,
âˆ’8.4%inference latency, âˆ’6.9%inference memory) by compressing
long behavioral histories into compact query sets; 2)Cross-request
KV cachingeliminates redundant sequence-side computation, re-
ducing runtime/latency by âˆ¼30%and memory by âˆ¼50%in bothtraining and serving; 3)FlashAttentionprimarily benefits train-
ing, reducing runtime by âˆ¼50%and activation memory by âˆ¼58%.
Inference gains are modest ( âˆ¼11â€“12%for latency and memory), as
attention dominates training costs with larger batches and back-
propagation; 4)Mixed precision with recomputationdelivers
the largest serving gains: p99 latency improves by âˆ¼69%and infer-
ence memory byâˆ¼30%, as inference can operate end-to-end in low
precision. By contrast, training must retain full-precision optimizer
states and gradient accumulators; even so, training runtime and
memory improve byâˆ¼32% andâˆ¼49%.
These results demonstrate the effectiveness of LLM optimiza-
tions for large-scale recommendation. Building on the ablations
conducted onOneTrans S, we scale up toOneTrans Land show that
with these techniques,OneTrans Lmaintains online efficiency com-
parable to the much smaller DCNv2+DIN baseline (Table 4). This
demonstrates again that reformulating RecSys into aunifiedTrans-
former backbone enables seamless adoption of LLM optimizations,
unlocking effective scaling previously unattainable in traditional
encode-then-interactionarchitectures.
4.5 RQ4: Scaling-Law Validation
We probescaling lawsforOneTransalong three axes: (1)length-
input token sequence length, (2)depth- number of stacked blocks,
and (3)width- hidden-state dimensionality.
As shown in Fig. 3(a), increasinglengthyields the largest gains
by introducing more behavioral evidence. Betweendepthandwidth,
we observe a clear trade-off: increasingdepthgenerally delivers
larger performance improvements than simply wideningwidth, as
deeper stacks extract higher-order interactions and richer abstrac-
tions. However, deeper models also increase serial computation,
whereas widening is more amenable to parallelism. Thus, choos-
ing betweendepthandwidthshould balance performance benefits
against system efficiency under the target hardware budget.
We further analyze scaling-law behavior by jointly widening
and deepeningOneTrans, and â€” for comparison â€” by scaling
theRankMixer+Transformerbaseline on the RankMixer side
till 1B; we then plot Î”UAUC versus training FLOPs on a log scale.
As shown in Fig. 3(b),OneTransand RankMixer both exhibit
clear log-linear trends, butOneTransshows asteeperslope, likely
because RankMixer-centric scaling lacks a unified backbone and
its MoE-based expansion predominantly widens the FFN hidden
dimension. Together, these results suggest thatOneTransis more
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Zhang et al.
Table 5: Impact of variants against the unoptimizedOneTrans S. Memory is peak GPU usage.
VariantTraining Inference
Runtime (ms) Memory (GB) Latency (p99; ms) Memory (GB)
Unoptimized OneTrans S 407 53.13 54.00 1.70
+ Pyramid stack âˆ’28.7%âˆ’42.6% âˆ’8.4%âˆ’6.9%
+ Cross-Request KV Caching âˆ’30.2%âˆ’58.4% âˆ’29.6%âˆ’52.9%
+ FlashAttention âˆ’50.1%âˆ’58.9% âˆ’12.3%âˆ’11.6%
+ Mixed Precision with Recomputation âˆ’32.9%âˆ’49.0% âˆ’69.1%âˆ’30.0%
2 4 6 8 10
TFLOPs (T)0.400.450.500.550.600.65 CTR UAUC (%)
T=768T=1024T=2048
L=6L=8L=10
D=256D=384D=512Length (#Tokens)
Depth (#Layers)
Width (#Dim)
(a) Trade-off: FLOPs vs.Î”UAUC
212223
TFLOPs (T, log scale)0.000.400.801.201.60 CTR UAUC (%)
OneTrans
RankMixer
(b) Scaling law:Î”UAUC vs. FLOPs (log)
Figure 3: Comparison of trade-off and scaling law.
parameter- and compute-efficient, offering favorable performanceâ€“
compute trade-offs for industrial deployment.
4.6 RQ5: Online A/B Tests
We assess the business impact ofOneTransin two large-scale
industrial scenarios: (i)Feeds(home feeds), and (ii)Mall(the overall
setting that includes Feeds and other sub-scenarios). Traffic is split
at the user/account level with hashing and user-level randomization.
Both thecontrolandtreatmentmodels are trained and deployed with
the past 1.5 years of production data to ensure a fair comparison.
Our prior production baseline,RankMixer+Transformer, serves
as thecontrol(â‰ˆ100M neural-network parameters) and does not
use sequence KV caching. ThetreatmentdeploysOneTrans Lwith
the serving optimizations described in Section 3.5.Table 6: Online A/B results:OneTrans L(treatment) vs.
RankMixer+Transformer (control). Order/u and GMV/u are
relative deltas (%). Latency is therelativeend-to-end per-
impression change Î”%(lower is better). * denotes ğ‘< 0.05,
and ** forğ‘<0.01
Scenario order/u gmv/u Latency (Î”%;ğ‘99)â†“
Feeds+4.3510%*+5.6848%*âˆ’3.91%
Mall+2.5772%**+3.6696%*âˆ’3.26%
We report user-levelorder/uandgmv/uas relative deltas ( Î”%)
versus theRankMixer+Transformercontrol with two-sided 95%
CIs (user-level stratified bootstrap), andend-to-end latencyâ€”
measured as the relative change in p99 per-impression time from
request arrival to response emission ( Î”%; lower is better). As shown
in Table 6,OneTrans Ldelivers consistent gains: inFeeds, +4.3510%
order/u ,+5.6848% gmv/u , andâˆ’3.91%latency; inMall, +2.5772%
order/u ,+3.6696% gmv/u , andâˆ’3.26%latency â€” indicating the
unified modeling framework lifts business metrics while reducing
serving time relative to a strong non-unified baseline.
We further observe a +0.7478%increase in userActive Daysand
a significant improvement of +13.59%incold-start product order/u,
highlighting the strong generalization capability of the proposed
model.
5 Conclusion
We presentOneTrans, a unified Transformer backbone for person-
alized ranking to replace the conventionalencodeâ€“thenâ€“interaction.
A unified tokenizer converts both sequential and non-sequential at-
tributes into one token sequence, and a unified Transformer block
jointly performs sequence modeling and feature interaction via
shared parameters for homogeneous (sequential) tokens and token-
specific parameters for heterogeneous (non-sequential) tokens. To
make the unified stack efficient at scale, we adopt a pyramid sched-
ule that progressively prunes sequential tokens and a cross-request
KV Caching that reuses user-side computation; the design further
benefits from LLM-style systems optimizations (e.g., FlashAtten-
tion, mixed precision). Across large-scale evaluations,OneTrans
exhibits near log-linear performance gains as width/depth increase,
and delivers statistically significant business lifts while maintain-
ing production-grade latency. We believe this unified design offers
a practical way to scale recommender systems while reusing the
system optimizations that have powered recent LLM advances.
OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
References
[1]Zheng Chai, Qin Ren, Xijun Xiao, Huizhi Yang, Bo Han, Sijun Zhang, Di Chen,
Hui Lu, Wenlin Zhao, Lele Yu, et al .2025. LONGER: Scaling Up Long Sequence
Modeling in Industrial Recommenders.arXiv preprint arXiv:2505.04421(2025).
[2]Jianxin Chang, Chenbin Zhang, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song,
and Kun Gai. 2023. Pepnet: Parameter and embedding personalized network for
infusing with personalized prior information. InProceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 3795â€“3804.
[3]Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, et al .2010. Training
and testing low-degree polynomial data mappings via linear svm.Journal of
Machine Learning Research11, 4 (2010).
[4]Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Be-
havior Sequence Transformer for E-commerce Recommendation in Alibaba.
arXiv:1905.06874 [cs.IR] https://arxiv.org/abs/1905.06874
[5]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan
Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah.
2016. Wide & Deep Learning for Recommender Systems. arXiv:1606.07792 [cs.LG]
https://arxiv.org/abs/1606.07792
[6]Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks
for youtube recommendations. InProceedings of the 10th ACM conference on
recommender systems. 191â€“198.
[7]Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. 2022. Flashat-
tention: Fast and memory-efficient exact attention with io-awareness.Advances
in neural information processing systems35 (2022), 16344â€“16359.
[8]Tri Dao, Aleksander Thomas, Anima Anandkumar, Matei Zaharia, and Christo-
pher Re. 2023. FlashAttention-2: Faster Attention with Better Parallelism and
Work Partitioning.arXiv preprint arXiv:2307.08691(2023).
[9]Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping
Yang. 2019. Deep Session Interest Network for Click-Through Rate Prediction.
arXiv:1905.06482 [cs.IR] https://arxiv.org/abs/1905.06482
[10] Audrunas Gruslys, Remi Munos, Ivo Daniel, Oriol Vinyals, and Koray
Kavukcuoglu. 2016. Memory-Efficient Backpropagation through Time. InAd-
vances in Neural Information Processing Systems (NeurIPS).
[11] Huan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong,
and Ed H. Chi. 2023. Hiformer: Heterogeneous Feature Interactions Learning
with Transformers for Recommender Systems. arXiv:2311.05884 [cs.IR] https:
//arxiv.org/abs/2311.05884
[12] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He, and Zhenhua
Dong. 2018. DeepFM: An End-to-End Wide & Deep Learning Framework for
CTR Prediction. arXiv:1804.04950 [cs.IR] https://arxiv.org/abs/1804.04950
[13] Junjie Huang, Jizheng Chen, Jianghao Lin, Jiarui Qin, Ziming Feng, Weinan
Zhang, and Yong Yu. 2024. A comprehensive survey on retrieval methods in
recommender systems.arXiv preprint arXiv:2407.21022(2024).
[14] Wang-Cheng Kang and Julian McAuley. 2018. Self-Attentive Sequential Recom-
mendation. arXiv:1808.09781 [cs.IR] https://arxiv.org/abs/1808.09781
[15] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,
Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models.arXiv preprint arXiv:2001.08361(2020).
[16] Shichen Liu, Fei Xiao, Wenwu Ou, and Luo Si. 2017. Cascade ranking for opera-
tional e-commerce search. InProceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. 1557â€“1565.
[17] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich
Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
Venkatesh, et al .2017. Mixed precision training.arXiv preprint arXiv:1710.03740
(2017).
[18] Paulius Micikevicius, Sharan Narang, Jonah Alben, Greg Diamos, Erich Elsen,
David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
Venkatesh, and Hao Wu. 2018. Mixed Precision Training. InInternational Confer-
ence on Learning Representations (ICLR).
[19] Nikil Pancha, Andrew Zhai, Jure Leskovec, and Charles Rosenberg. 2022. Pinner-
former: Sequence modeling for user representation at pinterest. InProceedings
of the 28th ACM SIGKDD conference on knowledge discovery and data mining.
3702â€“3712.
[20] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang
Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong
sequential behavior data for click-through rate prediction. InProceedings of the
29th ACM International Conference on Information & Knowledge Management.
2685â€“2692.
[21] Jiarui Qin, Jiachen Zhu, Bo Chen, Zhirong Liu, Weiwen Liu, Ruiming Tang, Rui
Zhang, Yong Yu, and Weinan Zhang. 2022. Rankflow: Joint optimization of multi-
stage cascade ranking systems as flows. InProceedings of the 45th International
ACM SIGIR Conference on Research and Development in Information Retrieval.
814â€“824.[22] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,
and Jian Tang. 2019. AutoInt: Automatic Feature Interaction Learning via Self-
Attentive Neural Networks. InProceedings of the 28th ACM International Confer-
ence on Information and Knowledge Management (CIKM â€™19). ACM, 1161â€“1170.
doi:10.1145/3357384.3357925
[23] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.
2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Rep-
resentations from Transformer. arXiv:1904.06690 [cs.IR] https://arxiv.org/abs/
1904.06690
[24] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network
for Ad Click Predictions. arXiv:1708.05123 [cs.LG] https://arxiv.org/abs/1708.
05123
[25] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,
and Ed Chi. 2021. DCN V2: Improved Deep & Cross Network and Practical
Lessons for Web-scale Learning to Rank Systems. InProceedings of the Web
Conference 2021 (WWW â€™21). ACM, 1785â€“1797. doi:10.1145/3442381.3450078
[26] Xue Xia, Pong Eksombatchai, Nikil Pancha, Dhruvil Deven Badani, Po-Wei Wang,
Neng Gu, Saurabh Vishwas Joshi, Nazanin Farahpour, Zhiyuan Zhang, and An-
drew Zhai. 2023. Transact: Transformer-based realtime user action model for
recommendation at pinterest. InProceedings of the 29th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining. 5249â€“5259.
[27] Zhichen Zeng, Xiaolong Liu, Mengyue Hang, Xiaoyi Liu, Qinghai Zhou, Chaofei
Yang, Yiqun Liu, Yichen Ruan, Laming Chen, Yuxin Chen, et al .2024. Interformer:
Towards effective heterogeneous interaction learning for click-through rate
prediction.arXiv preprint arXiv:2411.09852(2024).
[28] Buyun Zhang, Liang Luo, Yuxin Chen, Jade Nie, Xi Liu, Daifeng Guo, Yanli Zhao,
Shen Li, Yuchen Hao, Yantao Yao, et al .2024. Wukong: Towards a scaling law for
large-scale recommendation.arXiv preprint arXiv:2403.02545(2024).
[29] Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization.
Advances in neural information processing systems32 (2019).
[30] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu,
and Kun Gai. 2018. Deep Interest Evolution Network for Click-Through Rate
Prediction. arXiv:1809.03672 [stat.ML] https://arxiv.org/abs/1809.03672
[31] Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma,
Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for
Click-Through Rate Prediction. arXiv:1706.06978 [stat.ML] https://arxiv.org/abs/
1706.06978
[32] Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai.
2018. Learning tree-based deep model for recommender systems. InProceedings
of the 24th ACM SIGKDD international conference on knowledge discovery & data
mining. 1079â€“1088.
[33] Jie Zhu, Zhifang Fan, Xiaoxie Zhu, Yuchen Jiang, Hangyu Wang, Xintian Han,
Haoran Ding, Xinmin Wang, Wenlin Zhao, Zhen Gong, Huizhi Yang, Zheng Chai,
Zhe Chen, Yuchao Zheng, Qiwei Chen, Feng Zhang, Xun Zhou, Peng Xu, Xiao
Yang, Di Wu, and Zuotao Liu. 2025. RankMixer: Scaling Up Ranking Models in
Industrial Recommenders. arXiv:2507.15551 [cs.IR] https://arxiv.org/abs/2507.
15551