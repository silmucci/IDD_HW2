TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation

TV-Rec: Time-Variant Convolutional Filter for
Sequential Recommendation
Yehjin Shin Jeongwhan Choi Seojin Kim Noseong Parkâˆ—
KAIST
{yehjin.shin, jeongwhan.choi, seojinkim, noseong}@kaist.ac.kr
Abstract
Recently, convolutional filters have been increasingly adopted in sequential rec-
ommendation for their ability to capture local sequential patterns. However, most
of these models complement convolutional filters with self-attention. This is be-
cause convolutional filters alone, generally fixed filters, struggle to capture global
interactions necessary for accurate recommendation. We proposeTime-Variant
Convolutional Filters for SequentialRecommendation (TV-Rec), a model inspired
by graph signal processing, where time-variant graph filters capture position-
dependent temporal variations in user sequences. By replacing both fixed kernels
and self-attention with time-variant filters, TV-Rec achieves higher expressive
power and better captures complex interaction patterns in user behavior. This de-
sign not only eliminates the need for self-attention but also reduces computation
while accelerating inference. Extensive experiments on six public benchmarks
show that TV-Rec outperforms state-of-the-art baselines by an average of 7.49%.
1 Introduction
Recommender systems have become essential for guiding users through vast amounts of content
by providing personalized information based on usersâ€™ historical interactions [ 39,12,1,18,14,
2,3,7,15,20]. Considering that preferences evolve over time, sequential recommendation (SR)
has become widely used for capturing dynamic preferences using sequential patterns in usersâ€™
interactions. Various approaches have been developed to more accurately capture usersâ€™ dynamic
sequential patterns, including architectures such as Markov chains [ 24], RNNs [ 13], and GNNs [ 34].
Transformers [ 31], a powerful architecture in NLP, have been widely adopted as encoders for many
SR models, highlighting their ability to model long-term dependencies in data [17, 29, 23, 5].
Table 1: Comparison of existing methods based on three
points: i) convolutional filter type, ii) inference efficiency,
and iii) recommendation performance. The double tick mark
indicates better performance compared to a single tick mark.
ModelConvolutional Self- Inference Rec.
Filter Attention Efficiency Performance
SASRec [17]âœ— âœ“ âœ“ âœ“
BERT4Rec [29]âœ— âœ“ âœ“ âœ“
Caser [30] Fixedâœ— âœ— âœ“
NextItNet [40] Fixedâœ— âœ— âœ“
FMLPRec [45] Fixedâœ— âœ“ âœ“
AdaMCT [16] Fixedâœ“ âœ— âœ“âœ“
BSARec [28] Fixedâœ“ âœ— âœ“âœ“
TV-Rec (Ours) Time-Variantâœ— âœ“ âœ“âœ“Despite their modeling power, self-
attention mechanisms have a funda-
mental limitation: they lack an induc-
tive bias toward sequential structure.
While position embeddings provide
absolute position information, self-
attention still treats all positions pair-
wise without any inherent bias toward
local proximity. As a result, it makes
it difficult to model fine-grained, local-
ized user behavior patterns [ 28]. This
limitation has led to the development
of hybrid models that integrate con-
volutional layers to introduce locality
âˆ—Corresponding author.
39th Conference on Neural Information Processing Systems (NeurIPS 2025).arXiv:2510.25259v1  [cs.IR]  29 Oct 2025
bias. For instance, AdaMCT [ 16] utilizes self-attention and 1D convolution together to capture
both long-term and short-term user preferences, while BSARec [ 28] addresses the limitations of
self-attention by applying convolution using the Fourier Transform.
At the same time, convolution-only models have been explored for their effectiveness in capturing
local sequential dependencies, which are particularly valuable in SR. As shown in Table 1, models
such as Caser [ 30], NextItNet [ 40], and FMLPRec [ 45] use fixed convolutional filters to detect
patterns in user sequences. These filters focus on usersâ€™ recent behavior, which is advantageous
in recommending usersâ€™ next items. However, their fixed nature limits their adaptability: the same
filter is applied uniformly across positions, making it difficult to capture temporally evolving or
position-specific semantics. While convolutional filters solve this issue by using multiple fixed filters
to capture various patterns, these filters remain static and cannot adapt to the specific context or
temporal variations at each position in the sequence. This limits their expressiveness in modeling
evolving user preferences, especially when user interests shift rapidly over time.
This reveals a fundamental trade-off: convolution excels at modeling local patterns but lacks flexibility,
whereas self-attention is expressive but inefficient and insensitive to locality. To bridge this gap, we
propose a new architecture calledTime-Variant Convolutional Filters for SequentialRecommendation
(TV-Rec), which captures both local and global patterns while achieving greater efficiency than
existing hybrid models such as AdaMCT and BSARec. We design time-variant convolutional filters
to effectively capture temporal variations and emphasize the most relevant elements at each time step.
Our key finding is that existing complicated models based on the self-attention and convolutional
filter can be replaced with our time-variant convolutional filter.
Inspired by graph signal processing (GSP), we reinterpret SR as a line graph, and instead of using
fixed convolution filters, we apply time-variant graph filters, analogous to node-variant filters in the
graph domain. These filters enable us to effectively adapt weights across the sequence, eliminating the
need for positional embeddings while directly encoding temporal signals. Moreover, the time-variant
filters act as linear operators, resulting in faster inference with lower complexity.
To evaluate the effectiveness of TV-Rec, we conduct extensive experiments on 6 benchmark datasets.
Our results indicate that TV-Rec consistently outperforms state-of-the-art baseline methods. Addi-
tionally, we perform a series of experiments comparing the theoretical and practical complexity of
TV-Rec with other recent hybrid baselines that combine convolution and self-attention. These experi-
ments demonstrate improved recommendation accuracy and enhanced generalization capabilities.
The contributions of this work are as follows:
â€¢We proposeTime-Variant Convolutional Filters for SequentialRecommendation (TV-Rec),
using time-variant convolutional filters to capture temporal dynamics and user behavior
patterns more effectively (Sec. 3).
â€¢We show that TV-Rec provides more expressive generalization (Sec. 3.4) and effectively
captures both long-term preferences and recent interests via filters designed as functions of
time (Sec. 4.5).
â€¢We conduct extensive experiments on 6 benchmark datasets, and the results demonstrate that
TV-Rec outperforms state-of-the-art baseline methods by an average of 7.49% (Sec. 4.2),
while achieving the optimal balance between accuracy and efficiency (Sec. 4.6).
2 Preliminaries
In this section, we present the problem statement and the notations used in this paper. We then discuss
GSP and the node-variant graph filter, which are core components of TV-Rec.
2.1 Problem Statement
SR aims to model user behavior sequences based on implicit feedback to predict and recommend the
userâ€™s next interaction. Assume that we have a set of users Uand a set of items V, where |U|and|V|
denote the total numbers of users and items, respectively. The interacted items of each user uâˆˆ U can
be chronologically ordered into a sequence S(u)= [v(u)
1, v(u)
2, . . . , v(u)
|S(u)|], where v(u)
irepresents the
i-th item in the sequence of user u. For simplicity, the superscript (u), which indicates the user, will
2
be omitted henceforth. Therefore, the goal is to predict p(v|S|+1 =v| S) and recommend a Top- r
list of items as potential next interactions in the sequence.
2.2 Graph Signal Processing (GSP)
Our method, TV-Rec, incorporates key concepts from GSP. GSP analyzes signals on graphs, with
graph filtering as a core operation that emphasizes or suppresses specific frequency components of
the signal. Given a shift operator SâˆˆRNÃ—N, which can be adjacency matrix or Laplacian matrix, a
graph filterGis defined as a polynomial ofS:
y=Gx=KX
k=0hkSkx,(1)
wherexâˆˆRNis a graph signal,h kare filter taps andKis the order of the filter.
This operation can be interpreted in the frequency domain using the graph Fourier transform (GFT),
which enables the decomposition of graph signals into different frequency components. Given a shift
operatorS, the GFT is defined using its eigen-decomposition2:
S=Udiag(Î»)UâŠ¤,(2)
where Uis the matrix of eigenvectors, Î»is the vector of eigenvalues, and diag (Â·) indicates construct-
ing a diagonal matrix from a vector. The GFT of a graph signal xisËœx=UâŠ¤x, and the inverse GFT
isx=U Ëœx. In the frequency domain, the graph filterGacts on the transformed signal Ëœx:
Ëœy= ËœGËœx=KX
k=0hkdiag(Î»)k
Ëœx,(3)
where ËœGforms a diagonal matrix, allowing the calculation between the graph filter and the signal to
be element-wise multiplication. The filtered signal in the time domain can be obtained by applying
inverse GFT,y=U Ëœy.
In the context of SR, GSP can be utilized to model user behavior sequences as graph signals. In this
approach, each item in a sequence is represented as a node, with edges pointing to the next item,
forming a line graph. By applying graph filters, we can capture the complex dependencies between
items and enhance the predictive performance of SR. Graph convolution in this setting aggregates
information from neighboring items in the sequence, which are close in time, allowing the model to
learn both local and global patterns in user interactions.
2.3 Node-Variant Graph Filter
We focus on node-variant graph filters [ 27,6], that apply distinct filter taps at each node position. As
shown in Fig. 1, conventional graph convolutional filters introduced above use a scalar as the filter
tap (h k), whereas node-variant graph filtersG nvuse a vector (h k) for the filter tap, as follows:
y=G nvx=KX
k=0diag(h k)Sk
x,(4)
where diag (Â·) indicates constructing a diagonal matrix from a vector, meaning that every node has
a different filter tap hk= [h(1)
k, h(2)
k,Â·Â·Â·, h(N)
k]. The set of filter taps can be represented in matrix
form asHâˆˆCNÃ—(K+1), where thek-th column ish k.
While conventional graph convolutional filters are calculated by element-wise multiplication of the
frequency responses of the graph signal and the filter, the frequency response of a node-variant graph
filter is given as follows:
Ëœy= ËœGnvËœx=UâŠ¤(Uâ—¦(HÎ›âŠ¤))Ëœx,(5)
2In the general case, the GFT considers the Jordan decomposition S=VJVâˆ’1, but we assume that Sis a
diagonalizable matrix, so the Jordan decomposition is equivalent to the eigen-decomposition.
3
ğ‘¥!ğ‘¥"ğ‘¥#ğ‘¦!ğ‘¦"ğ‘¦#00MultiplySumâ„!â„"â„#â„!â„"â„#â„!â„"â„#(a) Fixed Filter
ğ‘¥!ğ‘¥"ğ‘¥#ğ‘¦!ğ‘¦"0ğ‘¦#0MultiplySumâ„!(!)	â„"(!)â„#(!)â„!(")â„"(")â„#(")â„!(#)â„"(#)â„#(#) (b) Time-Variant Convolutional Filter
Figure 1: Comparison of a fixed filter in (a) and a time-variant convolutional filter in (b) under our
line graph expression of a sequence of signals xi, with K= 2 andN= 3 . The output yj, i.e., the
filtered signal at index j, is produced by summing the filtered results. Arrow colors show each filterâ€™s
contribution to the output yj, while different hibox colors represent different filters. In the fixed filter
case in (a), the same filter hiis applied to every node, while the time-variant convolutional filter in
(b) allows each node to have its own filter.
where Î›âˆˆCNÃ—(K+1)is a Vandermonde matrix3given by Î›ik=Î»kâˆ’1
iandâ—¦denotes the element-
wise product of matrices. The proof for the frequency response of the node-variant graph filter can be
found in Appendix A.
Node-variant graph filters provide a flexible, general approach to creating operators while preserving
local implementation, effectively adapting to changing user preferences. In our line graph context,
where each node represents a distinct time point, these filters function equivalently totime-variant
convolutional filters. For consistency, we will use this term in the following sections.
3 Proposed Method
Embedding LayerFilter LayerAdd & NormFeed ForwardAdd & NormPrediction Layer
ğ—"â„“=ğ”âˆ˜ğ‡ğš²"ğ”"ğ—â„“IGFTFilterGFTElement-wise multiplicationGFT matrix: ğ”!IGFT matrix: ğ”Graph Fourier Transform (GFT)=ğ’ğ”ğ”!
00ğœ†!0ğœ†"0ğœ†#00
ğ‘¢#ğ‘¢"ğ‘¢!ğ‘¢#ğ‘¢"ğ‘¢!ğœ†!#ğœ†!"ğœ†!1ğœ†"#ğœ†""ğœ†"1ğœ†##ğœ†#"ğœ†#1ğš²=ğ¿	Ã—diag(ğ›Œ)
Figure 2: Architecture of our proposed TV-Rec.In this section, we present the design
of TV-Rec. As shown in Fig. 2, TV-
Rec consists of 3 modules: embedding
layer, time-variant encoder, and pre-
diction layer.
3.1 Embedding Layer
We first convert a userâ€™s historical in-
teraction sequence Sto a fixed length
N. If|S| â‰¥N , we truncate the se-
quence keeping the most recent N
items, and if |S|< N , we pad
the sequence with zeros at the be-
ginning. This process results in a se-
quence of length N, denoted as s=
(s1, s2,Â·Â·Â·, s N). Using the item em-
bedding matrix EâˆˆR|V|Ã—Dwhere Dis the latent dimension size, we then apply a look-up operation
to obtain the embedding representation of the user sequence, followed by layer normalization and
dropout. This process produces the final embedding of the user sequence X0, serving as the input for
the time-variant encoder:
X0=Dropout(LayerNorm([E s1,Es2,Â·Â·Â·,E sN]âŠ¤)),(6)
where Evdenotes the embedding of item vfromE. Note that positional embedding is not necessary
due to the benefits of applying our time-variant convolutional filters.
3A Vandermonde matrix has rows formed by the powers of a set of values, with each element in the i-th row
andj-th column given byxjâˆ’1
i.
4
3.2 Time-Variant Encoder
We build our item encoder by stacking Ltime-variant encoding blocks, each containing a filter layer,
a feed-forward network, and a residual connection applied after both.
Filter Layer.In the â„“-th filter layer, with Xâ„“as the input, we perform a filtering operation, then
apply a residual connection and layer normalization. As shown in Fig. 2, we first transform Xâ„“into
the frequency domain as eXâ„“=UâŠ¤Xâ„“, where Udenotes the GFT matrix derived from a padded
directed cyclic graph (DCG), which we adopt in place of a line graph to ensure diagonalizability and
enable spectral filtering (see Appendix B for formal justification). Then, we calculate the time-variant
convolutional filter using the filter tap HâˆˆCNÃ—(K+1)and the Vandermonde matrix Î›âˆˆRNÃ—(K+1):
bXâ„“=G nveX= 
Uâ—¦(HÎ›âŠ¤)eXâ„“= 
Uâ—¦(HÎ›âŠ¤)
UâŠ¤Xâ„“(7)
whereâ—¦indicates element-wise multiplication. Note that the inverse GFT matrix Uis multiplied with
the filter earlier than it is with the frequency response of the signal eXâ„“. To enhance expressive power,
we construct the filter matrixHas follows:
H=C Â¯B=CB
âˆ¥Bâˆ¥ 2
,(8)
where CâˆˆRNÃ—mis the coefficient matrix that generates position-specific filters, and Â¯Bâˆˆ
CmÃ—(K+1)is the normalized basis matrix. The parameter mdetermines the number of basis vectors.
Since each node corresponds to a position in the sequence, Ccan be considered as a function of time.
For numerical stability, we normalizeBusing the L2 norm along each row.
After Eq. (7), we use a residual connection with dropout and layer normalization to prevent overfitting:
Fâ„“=LayerNorm(Xâ„“+Dropout( bXâ„“)).(9)
Feed Forward Layer.After the filter layer, we employ a feed-forward network for non-linearity:
bFâ„“=FFN(Fâ„“) = (GELU(Fâ„“Wâ„“
1+bâ„“
1))Wâ„“
2+bâ„“
2,(10)
where Wâ„“
1,Wâ„“
2âˆˆRDÃ—D, andbâ„“
1,bâ„“
2âˆˆRDÃ—Dare learnable parameters. As in Eq. (9), we apply a
dropout layer, residual connections, and layer normalization to get the output of â„“â€™s layer as follows:
Xâ„“+1=LayerNorm(Fâ„“+Dropout( bFâ„“)).(11)
3.3 Prediction Layer and Training
Prediction Layer.After processing through Ltime-variant encoding blocks, we compute the userâ€™s
preference score for each item in the entire item setVas follows:
Ë†yv=p(v |S|+1 =v|S) =EâŠ¤
vXL
N,(12)
whereE vis the embedding of itemvandXL
Nis the final sequence representation.
Model Training.Similar to other studies [ 16,28,23], we optimize our model using cross-entropy
lossL cewith an orthogonal regularization termL orthoon the basis matrixBused in Eq. (8):
L=âˆ’logexp(Ë†y g)P
vâˆˆVexp(Ë†y v)| {z }
Lce+Î±Â·BrealBâŠ¤
realâˆ’I2
F+BimagBâŠ¤
imagâˆ’I2
F
| {z }
Lortho,(13)
where gis the ground-truth item, BrealandBimagdenote the real and imaginary components of B,Î±
controls the regularization strength, Idenotes the identity matrix, and Fdenotes the Frobenius norm.
3.4 Discussion
Relations to Other Methods.Several existing SR methods can be viewed as special cases within
our time-variant filter. 1D CNN in AdaMCT corresponds to a fixed graph convolutional filter, G,
defined in Eq. (1), where Krepresents the kernel size of the CNN. Similarly, the filter layers in
FMLPRec and BSARec apply the discrete Fourier transform (DFT), mathematically equivalent to the
GFT of a DCG [ 26,25,28], representable as ËœGin Eq. (3). Our time-variant filter is a more general
method that can be reduced to these approaches as special cases when the filters are fixed.
5
Comparison to GNN-based Methods.TV-Rec relates to GNN-based methods that model depen-
dencies between items through item-transition graphs [ 33,35,43]. Despite sharing a graph-based
formulation, TV-Rec differs from these GNN-based approaches in two key aspects. First, TV-Rec
defines sequence positions rather than items as graph nodes, allowing repeated items at different
temporal positions to be treated distinctly. This enables the model to capture fine-grained temporal
and positional dependencies that are often ignored when identical items are merged in a graph.
Second, while GNN-based models rely on iterative message passing to update node embeddings,
TV-Rec employs time-variant graph convolutional filters that directly operate in the spectral domain
without recursive propagation. This design removes the need for heavy message passing, yielding
a more efficient yet expressive representation of temporal dynamics. In this sense, TV-Rec can be
viewed as a non-GNN graph-based paradigm that bridges graph signal processing and sequential
recommendation.
Why We Need Time-Variant Graph Filters?In Fig. 1, a fixed filter applies the same weights in
the sequence, emphasizing recent items. However, it also makes it difficult to capture specific patterns
at different stages. For instance, while the patterns at the end of the sequence may highlight recent
items, the patterns at the beginning can provide crucial insights into the userâ€™s overall preferences. As
a result, the fixed filter may lose valuable information, particularly when attempting to understand
early-stage patterns. In contrast, our time-variant filter uses different filters for each position, allowing
the model to capture a broader range of patterns. This includes both recent items and long-term
preferences. Unlike fixed filters that focus predominantly on recent items, the time-variant filter can
adapt to shifts and unique characteristics at various positions, improving overall performance.
Why Positional Encoding is Unnecessary?Unlike Transformer-based models that require explicit
positional encodings (e.g., sinusoidal or learnable), TV-Rec naturally encodes positional information
through the spectral properties of the graph. Since TV-Rec constructs a directed cyclic graph and
applies the GFT, it inherently shares the same frequency components as sinusoidal encodings (see
Appendix C for details). Furthermore, the time-variant graph filter acts as a position-specific operation,
dynamically modulating frequency components without the need for additional embeddings. Ablation
studies (Sec. 4.3) confirm that adding explicit positional embeddings offers no performance benefit,
validating the modelâ€™s built-in, efficient positional encoding mechanism.
Time Complexity.Assume that nis the length of the input sequence and dis the dimension of
each input vector. The time complexity of self-attention is O(nd2+n2d), where O(nd2)is for
computing the key, query, and value matrices, and O(n2d)is for calculating the attention scores
and applying them to the value matrix. The time complexity of our time-variant convolutional filter
isO(n2m+n2d), where O(n2m)is for computing the filter tap H, and O(n2d)is for applying
GFT to the input signal and multiplying it with the filter tap. Since mâ‰¤n , this can be simplified to
O(n3+n2d). The difference in complexity between self-attention and the time-variant graph filter
depends on the relative sizes of nandd, as it determines which term dominates. It is worth noting
that the time-variant convolutional filter is a linear operator, so Gnvdoes not need to be computed
for every inference, and can be precomputed after training, resulting in a time complexity of O(n2d).
4 Experiments
4.1 Experimental Setup
We evaluate TV-Rec on six benchmark datasets for SR, following the preprocessing procedures in
[45,44]. We compare our model with state-of-the-art baseline methods including conventional and
recent hybrid SR models. For standard experiments, we set the maximum sequence length Nto
50. Additionally, to examine performance on long-range dependencies, we conduct experiments on
ML-1M and Foursquare with longer average interaction with Nset to 200. For evaluation, we use
standard Top- rmetrics (HR@ rand NDCG@ rforrâˆˆ {5,10,20} ) computed over the entire item set
without negative sampling [ 19]. Detailed experimental setups including dataset statistics and optimal
hyperparameter configurations are provided in Appendix D. The source code is publicly available at
https://github.com/yehjin-shin/TV-Rec.
6
Table 2: Performance comparison of different methods. The best results are inboldfaceand the
second-best results are underlined . â€˜Improv.â€™ indicates the relative improvement against the best
baseline performance.
Datasets Metric Caser GRU4Rec SASRec BERT4Rec NextItNet FMLPRec DuoRec LRURec AdaMCT BSARec TV-Rec Improv.
BeautyHR@5 0.0149 0.0170 0.0368 0.0491 0.0549 0.0423 0.0680 0.0648 0.0675 0.0714 0.07210.98%
HR@10 0.0253 0.0307 0.0574 0.0742 0.0779 0.0639 0.0944 0.0889 0.0925 0.0990 0.10172.73%
HR@20 0.0416 0.0499 0.0860 0.1079 0.1100 0.0949 0.1279 0.1197 0.1299 0.1393 0.14030.72%
NDCG@5 0.0089 0.0105 0.0241 0.0318 0.0392 0.0272 0.0485 0.0472 0.0489 0.0501 0.05132.40%
NDCG@10 0.0122 0.0149 0.0307 0.0399 0.0467 0.0341 0.0570 0.0549 0.0569 0.0590 0.06083.05%
NDCG@20 0.0164 0.0198 0.0379 0.0484 0.0547 0.0419 0.0654 0.0627 0.0664 0.0691 0.07052.03%
SportsHR@5 0.0091 0.0131 0.0215 0.0279 0.0311 0.0222 0.0390 0.0351 0.0386 0.0422 0.04312.13%
HR@10 0.0147 0.0211 0.0319 0.0434 0.0458 0.0358 0.0549 0.0502 0.0544 0.0623 0.06351.93%
HR@20 0.0253 0.0347 0.0485 0.0658 0.0682 0.0549 0.0779 0.0698 0.0769 0.0865 0.08801.73%
NDCG@5 0.0064 0.0084 0.0142 0.0182 0.0212 0.0148 0.0276 0.0242 0.0272 0.0296 0.02980.68%
NDCG@10 0.0082 0.0110 0.0175 0.0232 0.0260 0.0191 0.0328 0.0291 0.0322 0.0361 0.03630.55%
NDCG@20 0.0109 0.0144 0.0217 0.0288 0.0316 0.0239 0.0385 0.0340 0.0379 0.0422 0.04250.71%
YelpHR@5 0.0131 0.0137 0.0165 0.0243 0.0247 0.0195 0.0277 0.0240 0.0239 0.0260 0.02904.69%
HR@10 0.0230 0.0240 0.0267 0.0411 0.0423 0.0313 0.0450 0.0396 0.0404 0.0446 0.04745.33%
HR@20 0.0388 0.0412 0.0445 0.0681 0.0694 0.0518 0.0730 0.0656 0.0670 0.0718 0.07776.44%
NDCG@5 0.0080 0.0086 0.0103 0.0154 0.0151 0.0122 0.0179 0.0151 0.0153 0.0162 0.01863.91%
NDCG@10 0.0112 0.0119 0.0135 0.0208 0.0208 0.0160 0.0234 0.0201 0.0206 0.0222 0.02454.70%
NDCG@20 0.0151 0.0162 0.0180 0.0275 0.0276 0.0211 0.0304 0.0266 0.0272 0.0290 0.03215.59%
LastFMHR@5 0.0303 0.0339 0.0422 0.0358 0.0431 0.0450 0.0404 0.0358 0.0468 0.0505 0.059618.02%
HR@10 0.0459 0.0394 0.0670 0.0606 0.0624 0.0670 0.0587 0.0532 0.0716 0.0679 0.085319.13%
HR@20 0.0606 0.0550 0.0972 0.0908 0.0936 0.1000 0.0872 0.0807 0.1018 0.1119 0.12027.42%
NDCG@5 0.0222 0.0231 0.0301 0.0213 0.0264 0.0321 0.0276 0.0257 0.0330 0.0348 0.040215.52%
NDCG@10 0.0269 0.0249 0.0382 0.0291 0.0325 0.0392 0.0336 0.0312 0.0409 0.0405 0.048418.34%
NDCG@20 0.0306 0.0288 0.0458 0.0366 0.0402 0.0475 0.0407 0.0380 0.0485 0.0514 0.057211.28%
ML-1MHR@5 0.1033 0.1225 0.1406 0.1651 0.1858 0.1329 0.1821 0.1916 0.1773 0.1909 0.20135.06%
HR@10 0.1671 0.1925 0.2199 0.2442 0.2724 0.2089 0.2690 0.2848 0.2560 0.2798 0.29041.97%
HR@20 0.2598 0.2906 0.3250 0.3459 0.3853 0.3212 0.3757 0.3886 0.3647 0.3844 0.40794.97%
NDCG@5 0.0663 0.0779 0.0920 0.1077 0.1264 0.0861 0.1226 0.1339 0.1185 0.1286 0.13712.39%
NDCG@10 0.0868 0.1006 0.1174 0.1332 0.1543 0.1105 0.1507 0.1640 0.1438 0.1573 0.16581.10%
NDCG@20 0.1101 0.1253 0.1438 0.1588 0.1829 0.1388 0.1776 0.1901 0.1711 0.1836 0.19552.84%
FoursquareHR@5 0.0139 0.0148 0.0139 0.0139 0.0129 0.0120 0.0139 0.0148 0.0157 0.0148 0.017511.46%
HR@10 0.0175 0.0157 0.0185 0.0157 0.0175 0.0175 0.0185 0.0166 0.0185 0.0212 0.025922.17%
HR@20 0.0268 0.0231 0.0268 0.0231 0.0203 0.0240 0.0240 0.0212 0.0259 0.0277 0.031413.36%
NDCG@5 0.0099 0.0110 0.0102 0.0108 0.0093 0.0076 0.0110 0.0110 0.0105 0.0111 0.013420.72%
NDCG@10 0.0110 0.0113 0.0117 0.0113 0.0108 0.0094 0.0124 0.0116 0.0113 0.0130 0.016123.85%
NDCG@20 0.0133 0.0132 0.0137 0.0132 0.0115 0.0110 0.0139 0.0127 0.0131 0.0147 0.017619.73%
Table 3: Results of long-range modeling performance.
Datasets Metric Caser GRU4Rec SASRec BERT4Rec NextItNet FMLPRec DuoRec LRURec AdaMCT BSARec TV-Rec Improv.
ML-1MHR@5 0.1109 0.1518 0.1558 0.1730 0.1978 0.1397 0.1930 0.2233 0.1760 0.1949 0.22550.99%
HR@10 0.1869 0.2374 0.2399 0.2573 0.2882 0.2296 0.2795 0.3175 0.2619 0.2917 0.32321.80%
HR@20 0.2942 0.3455 0.3551 0.3695 0.3970 0.3462 0.3854 0.4205 0.3695 0.4005 0.43062.40%
NDCG@5 0.0696 0.0981 0.1014 0.1147 0.1334 0.0885 0.1292 0.1516 0.1167 0.1327 0.15723.69%
NDCG@10 0.0939 0.1256 0.1285 0.1418 0.1627 0.1175 0.1571 0.1820 0.1443 0.1639 0.18863.63%
NDCG@20 0.1209 0.1528 0.1576 0.1701 0.1901 0.1468 0.1838 0.2079 0.1715 0.1913 0.21573.75%
FoursquareHR@5 0.0139 0.0120 0.0111 0.0102 0.0083 0.0120 0.0120 0.0129 0.0120 0.0129 0.01486.47%
HR@10 0.0194 0.0157 0.0175 0.0157 0.0166 0.0148 0.0194 0.0139 0.0157 0.0175 0.02129.28%
HR@20 0.0231 0.0194 0.0295 0.0240 0.0259 0.0194 0.0286 0.0185 0.0305 0.0305 0.03235.90%
NDCG@5 0.0105 0.0099 0.0085 0.0078 0.0068 0.0087 0.0078 0.0099 0.0094 0.0089 0.01082.86%
NDCG@10 0.0123 0.0111 0.0106 0.0096 0.0095 0.0096 0.0102 0.0102 0.0106 0.0103 0.01294.88%
NDCG@20 0.0133 0.0120 0.0136 0.0117 0.0118 0.0108 0.0126 0.0114 0.0142 0.0135 0.015811.27%
4.2 Overall Performance
Sequential Recommendation Results.As shown in Table 2, TV-Rec outperforms all baseline
methods, with an average accuracy improvement of 7.49% over the strongest baselines. The im-
provements are particularly significant on LastFM (19.13% on HR@10 and 18.34% on NDCG@10)
and Foursquare (22.17% on HR@10 and 23.85% on NDCG@10). On larger datasets like ML-1M,
TV-Rec still shows gains with improvements of 5.06% on HR@5 and 2.39% on NDCG@5. However,
for E-commerce datasets like Beauty and Sports, the improvements are more subtle but still consistent
(0.98% and 2.13% on HR@5, respectively). Among the baselines, recent hybrid methods that combine
convolution and self-attention, such as AdaMCT and BSARec, show strong performance. BSARec
achieves the second-best results in many cases. For ML-1M, LRURec shows competitive results with
its specialized architecture for long sequences, while DuoRec achieves second-best performance for
Yelp. Despite these strong baselines, TV-Rec consistently outperforms them by significant margins.
7
Table 4: Results of performance comparison with GNN-based methods.
MethodsBeauty Sports Yelp LastFM ML-1M Foursquare
H@20 N@20 H@20 N@20 H@20 N@20 H@20 N@20 H@20 N@20 H@20 N@20
TV-Rec 0.1403 0.0705 0.0880 0.0425 0.0777 0.0321 0.1202 0.0572 0.4079 0.1955 0.0314 0.0176
SR-GNN 0.0847 0.0374 0.0517 0.0224 0.0609 0.0252 0.0872 0.0379 0.2940 0.1390 0.0222 0.0137
GC-SAN 0.1059 0.0546 0.0608 0.0289 0.0635 0.0260 0.0807 0.0394 0.3255 0.1611 0.0212 0.0131
GCL4SR 0.1206 0.0601 0.0744 0.0356 0.0684 0.0276 0.0908 0.0398 0.3381 0.1607 0.0185 0.0123
Table 5: Ablation studies.
MethodsBeauty Sports Yelp LastFM ML-1M Foursquare
H@20 N@20 H@20 N@20 H@20 N@20 H@20 N@20 H@20 N@20 H@20 N@20
TV-Rec0.14030.0705 0.0880 0.0425 0.0777 0.0321 0.1202 0.0572 0.4079 0.1955 0.0314 0.0176
(1) Positional Embedding0.14080.0702 0.0842 0.0396 0.0763 0.0320 0.1018 0.0496 0.4017 0.1936 0.0313 0.0160
(2) Basic Graph Filter 0.1402 0.0692 0.0857 0.0408 0.0747 0.0307 0.1165 0.0543 0.3974 0.1933 0.0212 0.0113
(3) Identity Basis 0.1400 0.0698 0.0851 0.0410 0.0765 0.0317 0.1138 0.0539 0.4015 0.1930 0.0277 0.0141
(4) Basis Normalization 0.1336 0.0689 0.0841 0.0412 0.0634 0.0264 0.0963 0.0418 0.3985 0.1912 0.0305 0.0144
Long-Range Sequential Recommendation Results.To examine the performance of TV-Rec on
long-range dependencies, we additionally conduct experiments on ML-1M and Foursquare, which
have a long average interaction length, by setting the maximum length Nto 200. As shown in Table 3,
TV-Rec outperforms all baseline models in long-range SR tasks. Our model achieves an average
improvement of 4.74% on all metrics compared to the top baseline performances. The improvement
is particularly significant in NDCG metrics, with TV-Rec showing up to 11.27% gain in NDCG@20
for Foursquare. For ML-1M, we observe that LRURec performs strongest among baselines due to
its linear recurrent unit designed for long sequences. For Foursquare, Caser and AdaMCT show the
highest baseline performance. The superior results of TV-Rec on these settings show the effectiveness
of our time-variant filters on extended user interaction histories. This shows the ability of our approach
to maintain recommendation accuracy even when processing sequences with hundreds of interactions.
Comparison to GNN-based Methods.To further examine its effectiveness, we also compare
TV-Rec with three representative GNN-based sequential recommendation models: SR-GNN [ 33],
GC-SAN [ 35], and GCL4SR [ 43], under the same experimental settings described in Section 4.1.
While most GNN-based recommendation methods are tailored for collaborative filtering on static
userâ€“item graphs, these GNN-based sequential models utilize item-transition graphs to model local
or global item dependencies. As shown in Table 4, TV-Rec consistently outperforms all GNN-based
sequential models, confirming the advantage of its time-variant filtering design. By operating directly
on sequence positions instead of propagating messages over item-transition graphs, TV-Rec achieves
a more efficient representation of temporal dependencies.
4.3 Ablation Studies
We conduct ablation studies to validate the design choices of TV-Rec. The results are shown in
Table 5. (1) First, we test an ablation model with added learnable positional embeddings. Unlike
recent SR models [ 28,5,16], TV-Rec performs effectively without positional embeddings because our
time-variant filter inherently captures position-specific information. The first ablation model yields
inconsistent results on all datasets. (2) Second, we define the second ablation model by replacing
our time-variant filter with a basic graph filter. This ablation model degrades the performance, which
confirms the effectiveness of our method as discussed in Sec. 4.5. (3) Third, to validate our filter
construction method (Eq. 8), we define the third ablation model by setting Bas an identity matrix,
which makes Hequal to C. This ablation model degrades results compared to our filter construction
method. (4) Fourth, we compare against the fourth ablation model that uses the basis matrix Bdirectly
without normalization, demonstrating the effectiveness of our normalization approach. These results
prove that each component of our design contributes to the superiority of TV-Rec.
8
4.4 Parameter Sensitivity
We analyze the sensitivity of the parameters mand dropout rate p, as shown in Figs. 3 and 4. All
other hyperparameters are fixed at their optimal values. Additional results, including sensitivity to
filter orderKand weight decayÎ±, are provided in Appendix E.
48163250
m0.066 0.068 0.070NDCG@20NDCG@20
HR@20
0.1360.1380.140
HR@20
(a) Beauty
48163250
m0.0460.051 0.058NDCG@20NDCG@20
HR@20
0.104 0.112 0.120
HR@20 (b) LastFM
Figure 3: Sensitivity to the number of basis vectors
m.
0.10.20.30.40.5
p0.0370.039 0.042NDCG@20NDCG@20
HR@20
0.0790.083 0.088
HR@20
(a) Sports
0.10.20.30.40.5
p0.0110.0140.017NDCG@20NDCG@20
HR@20
0.0260.028 0.031
HR@20 (b) Foursquare
Figure 4: Sensitivity to dropout ratep.Sensitivity to m.We employ a basis matrix
Bin the time-variant graph filter (Eq. (8)),
where mcontrols the number of basis vectors,
influencing both expressiveness and computa-
tional cost. As shown in Fig. 3, performance on
Beauty peaks at m= 32 but drops sharply for
smaller values, indicating the need for richer
representations. Conversely, LastFM achieves
its best results at m= 8 , with performance
degrading as mincreases, suggesting that a
compact representation suffices for this dataset.
Sensitivity to p.The effect of dropout rate p
on Sports and Foursquare is shown in Fig. 4.
For Sports, larger pvalues improve perfor-
mance, while for Foursquare, smaller pvalues
work better. Datasets with fewer interactions
tend to benefit from higher dropout rates, which
help prevent overfitting by encouraging more
general representations.
4.5 Analyzing Filter Behavior and Case Study
01020304050
Shift0
10
20
30
40
50Node
0.000.020.040.060.080.100.12
(a) Basic Graph Filter
01020304050
Shift0
10
20
30
40
50Node
0.000.050.100.150.20 (b) Time-Variant Filter
Figure 5: Visualization of learned graph filters on LastFM.
The x-axis denotes the number of shifts in graph convo-
lution, while the y-axis represents individual nodes, with
higher numbers indicating more recent time points.
Early-stageMid-stageRecentDramaComedyActionWestern
Time-Variant FilterFixed Filter
WesternWesternComedyWesternActionComedyWesternMovie Recommendation by
Figure 6: Case Study on ML-1M.To understand why our time-variant filter
works better than fixed basic graph filter
(Eq.(3)), we analyze their learned repre-
sentations via visualizations and a case
study. Fig. 5 reveals the key difference be-
tween approaches: the basic graph filter
applies the same filter to all nodes, while
our time-variant filter applies different
filters to each node.
Both filters assign higher weights to
lower shifts, meaning they give stronger
weights to recent items. However, unlike
the basic graph filter, the time-variant
convolutional filter starts with similar
weights for the early-stage nodes, reflect-
ing the modelâ€™s focus on understanding
overall patterns. As the sequence pro-
gresses, the filterâ€™s weights shift to em-
phasize recent items, allowing the filter to
capture temporal shifts more accurately.
This shows the time-variant filterâ€™s ability
to adapt to both early and recent changes,
boosting performance.
Our case study on ML-1M (see Fig. 6)
shows this advantage in practice. The ba-
sic graph filter focuses solely onWesternfilms from recent interactions, while our time-variant filter
captures both userâ€™s recentWesterninterest and their broaderComedypreference. These findings
confirm our statement that applying different filtering operations at different temporal positions is
important for effective sequential recommendation.
9
4.6 Model Complexity and Runtime Analyses
0.6 0.7 0.8 0.9 1.0 1.1
Inference Time (seconds)0.020.030.040.050.060.07NDCG@20
CaserSASRecBERT4RecNextItNet
FMLPRecAdaMCTBSARecTV-Rec
Caser
SASRec
BERT4Rec
NextItNetFMLPRec
AdaMCT
BSARec
TV-Rec
Figure 7: Comparison of model infer-
ence time and NDCG@20 on Beauty.
The size of each circle corresponds to
the number of parameters.To evaluate the efficiency of TV-Rec, we analyze the
number of parameters and inference time. The results
across the full dataset are provided in the Appendix G.
As shown in Fig. 7, TV-Rec achieves the best balance be-
tween performance and computational efficiency. Among
top-performing methods, TV-Rec has the fastest inference
time with the smallest number of parameters, compared to
hybrid models that combine convolution and self-attention,
such as AdaMCT and BSARec. Compared to SASRec and
BERT4Rec, which use only self-attention, TV-Rec pro-
vides faster inference time and superior recommendation
accuracy. While FMLPRec runs slightly faster with its sim-
ple architecture, it achieves this through a basic graph filter
that degrades recommendation quality. Considering the
improved performance of TV-Rec, the marginally higher
inference time than FMLPRec is acceptable.
5 Related Work
5.1 Sequential Recommendation
SR has evolved from early approaches that used Markov chains [ 24] and RNNs [ 13] to model
sequential dependencies. Transformer-based models such as SASRec [ 17] and BERT4Rec [ 29] use
self-attention to capture global dependencies and establish new performance benchmarks. Recent
advanced methods have emerged to address specific challenges and efficiency-performance trade-offs
in SR. FMLPRec [ 45] proposes a filter-enhanced MLP to eliminate frequency domain noise, while
FEARec [ 5] and DuoRec [ 23] use contrastive learning approaches for better sequence representa-
tion. AC-TSR [ 46] calibrates unreliable attention weights generated by Transformer-based models.
LRURec [ 41] explores linear recurrent units to balance efficiency and performance. In addition, SR
models [ 21,42,38,32] based on state space models [ 10,9,8] have been explored for potential in SR.
5.2 Hybrid Approaches and Convolution in SR
Recent research has shown that convolution-based methods can serve as competitive alternatives to
existing SR methods [ 30,36,37,45,16,28,4]. The first to use convolution in SR was Caser [ 30],
which treats user-item interactions as images for 2D convolutions, followed by NextItNet [ 40], which
uses dilated 1D convolutions. FMLPRec [ 45] incorporated Fourier transforms within an all-MLP
architecture to enhance sequence representations. AdaMCT [ 16] incorporates 1D convolution into
Transformer-based recommendation model to capture both long-term and short-term user preferences.
BSARec [ 28] has recently achieved state-of-the-art results by addressing the limitations of self-
attention through the application of convolution using the Fourier transform. However, these models
typically require self-attention to achieve optimal performance. Our work differs by introducing
time-variant convolutional filters that achieve high performance without relying on self-attention.
6 Conclusion
We focus on the inherent limitations of conventional convolutional filters. Since these filters are fixed,
they may struggle to capture the complex patterns required for SR. To address this issue, we introduce
TV-Rec, which uses time-variant convolutional filters that apply different filters to each data point.
TV-Rec achieves high performance without relying on self-attention, even in long-range modeling.
Our method also benefits from fast inference times due to its linear operator nature. We validated
the effectiveness and efficiency of TV-Rec through extensive experiments on 6 datasets. In future
work, we plan to explore both the theoretical and practical relationships between our time-variant
filter and recent advances in state space models, which have demonstrated strong connections with
convolutional filters. We leave limitations in Appendix J.
10
Acknowledgments
This work was partly supported by the Institute for Information & Communications Technology
Planning & Evaluation (IITP) grants funded by the Korean government (MSIT) (No. RS-2022-
II220113, Developing a Sustainable Collaborative Multi-modal Lifelong Learning Framework),
Samsung Electronics Co., Ltd. (No. G01240136, KAIST Semiconductor Research Fund (2nd)), and
Samsung Research Funding & Incubation Center of Samsung Electronics under Project Number
SRFC-IT2402-08.
References
[1]Jeongwhan Choi, Jinsung Jeon, and Noseong Park. LT-OCF: Learnable-time ode-based collab-
orative filtering. InProceedings of the 30th ACM international conference on information &
knowledge management, pages 251â€“260, 2021.
[2]Jeongwhan Choi, Seoyoung Hong, Noseong Park, and Sung-Bae Cho. Blurring-sharpening
process models for collaborative filtering. InProceedings of the 46th international ACM SIGIR
conference on research and development in information retrieval, pages 1096â€“1106, 2023.
[3]Jeongwhan Choi, Hyowon Wi, Chaejeong Lee, Sung-Bae Cho, Dongha Lee, and Noseong Park.
RDGCL: Reaction-diffusion graph contrastive learning for recommendation.arXiv preprint
arXiv:2312.16563, 2023.
[4]Minjin Choi, Hye-young Kim, Hyunsouk Cho, and Jongwuk Lee. Multi-intent-aware session-
based recommendation. InProceedings of the 47th International ACM SIGIR Conference on
Research and Development in Information Retrieval, pages 2532â€“2536, 2024.
[5]Xinyu Du, Huanhuan Yuan, Pengpeng Zhao, Jianfeng Qu, Fuzhen Zhuang, Guanfeng Liu,
Yanchi Liu, and Victor S Sheng. Frequency enhanced hybrid attention network for sequential
recommendation. InSIGIR, pages 78â€“88, 2023.
[6]Fernando Gama, Brendon G Anderson, and Somayeh Sojoudi. Node-variant graph filters in
graph neural networks. In2022 IEEE Data Science and Learning Workshop (DSLW), pages
1â€“6. IEEE, 2022.
[7]Chen Gao, Yu Zheng, Nian Li, Yinfeng Li, Yingrong Qin, Jinghua Piao, Yuhan Quan, Jianxin
Chang, Depeng Jin, Xiangnan He, et al. A survey of graph neural networks for recommender
systems: Challenges, methods, and directions.ACM Transactions on Recommender Systems, 1
(1):1â€“51, 2023.
[8]Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.
arXiv preprint arXiv:2312.00752, 2023.
[9]Albert Gu, Karan Goel, and Christopher RÃ©. Efficiently modeling long sequences with structured
state spaces.arXiv preprint arXiv:2111.00396, 2021.
[10] James D Hamilton. State-space models.Handbook of econometrics, 4:3039â€“3080, 1994.
[11] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context.Acm
transactions on interactive intelligent systems (tiis), 5(4):1â€“19, 2015.
[12] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng Wang. Lightgcn:
Simplifying and powering graph convolution network for recommendation. InSIGIR, 2020.
[13] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based
recommendations with recurrent neural networks. InICLR, 2016.
[14] Seoyoung Hong, Minju Jo, Seungji Kook, Jaeeun Jung, Hyowon Wi, Noseong Park, and Sung-
Bae Cho. TimeKit: A time-series forecasting-based upgrade kit for collaborative filtering. In
2022 IEEE International Conference on Big Data (Big Data), pages 565â€“574. IEEE, 2022.
11
[15] Seoyoung Hong, Jeongwhan Choi, Yeon-Chang Lee, Srijan Kumar, and Noseong Park. SVD-AE:
Simple autoencoders for collaborative filtering. InProceedings of the Thirty-Third International
Joint Conference on Artificial Intelligence, IJCAI-24, pages 2054â€“2062. International Joint
Conferences on Artificial Intelligence Organization, 2024. doi: 10.24963/ijcai.2024/227. URL
https://doi.org/10.24963/ijcai.2024/227.
[16] Juyong Jiang, Peiyan Zhang, Yingtao Luo, Chaozhuo Li, Jae Boum Kim, Kai Zhang, Senzhang
Wang, Xing Xie, and Sunghun Kim. Adamct: adaptive mixture of cnn-transformer for sequential
recommendation. InProceedings of the 32nd ACM International Conference on Information
and Knowledge Management, pages 976â€“986, 2023.
[17] Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. InICDM,
pages 197â€“206. IEEE, 2018.
[18] Taeyong Kong, Taeri Kim, Jinsung Jeon, Jeongwhan Choi, Yeon-Chang Lee, Noseong Park,
and Sang-Wook Kim. Linear, or non-linear, that is the question! InProceedings of the fifteenth
ACM international conference on web search and data mining, pages 517â€“525, 2022.
[19] Walid Krichene and Steffen Rendle. On sampled metrics for item recommendation. In
Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery
& data mining, pages 1748â€“1757, 2020.
[20] Chaejeong Lee, Jeongwhan Choi, Hyowon Wi, Sung-Bae Cho, and Noseong Park. SCONE:
A novel stochastic sampling to generate contrastive views and hard negative samples for
recommendation. InProceedings of the Eighteenth ACM International Conference on Web
Search and Data Mining, pages 419â€“428, 2025.
[21] Chengkai Liu, Jianghao Lin, Jianling Wang, Hanzhou Liu, and James Caverlee. Mamba4rec:
Towards efficient sequential recommendation with selective state space models.arXiv preprint
arXiv:2403.03900, 2024.
[22] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based
recommendations on styles and substitutes. InProceedings of the 38th international ACM
SIGIR conference on research and development in information retrieval, pages 43â€“52, 2015.
[23] Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. Contrastive learning for representation
degeneration problem in sequential recommendation. InWSDM, pages 813â€“823, 2022.
[24] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. Factorizing personalized
markov chains for next-basket recommendation. InTheWebConf (former WWW), pages 811â€“820,
2010.
[25] Aliaksei Sandryhaila and JosÃ© MF Moura. Discrete signal processing on graphs.IEEE
transactions on signal processing, 61(7):1644â€“1656, 2013.
[26] Aliaksei Sandryhaila and Jose MF Moura. Discrete signal processing on graphs: Frequency
analysis.IEEE Transactions on Signal Processing, 62(12):3042â€“3054, 2014.
[27] Santiago Segarra, Antonio G Marques, and Alejandro Ribeiro. Optimal graph-filter design and
applications to distributed linear network operators.IEEE Transactions on Signal Processing,
65(15):4117â€“4131, 2017.
[28] Yehjin Shin, Jeongwhan Choi, Hyowon Wi, and Noseong Park. An attentive inductive bias for
sequential recommendation beyond the self-attention. InProceedings of the AAAI Conference
on Artificial Intelligence, volume 38, pages 8984â€“8992, 2024.
[29] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. BERT4Rec:
Sequential recommendation with bidirectional encoder representations from transformer. In
CIKM, pages 1441â€“1450, 2019.
[30] Jiaxi Tang and Ke Wang. Personalized top-n sequential recommendation via convolutional
sequence embedding. InWSDM, pages 565â€“573, 2018.
12
[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. InNeurIPS, 2017.
[32] Yuda Wang, Xuxin He, and Shengxin Zhu. Echomamba4rec: Harmonizing bidirectional state
space models with spectral filtering for advanced sequential recommendation.arXiv preprint
arXiv:2406.02638, 2024.
[33] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. Session-based
recommendation with graph neural networks. InProceedings of the AAAI conference on
artificial intelligence, volume 33, pages 346â€“353, 2019.
[34] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. Session-based
recommendation with graph neural networks. InAAAI, volume 33, pages 346â€“353, 2019.
[35] Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Fuzhen Zhuang, Junhua
Fang, and Xiaofang Zhou. Graph contextualized self-attention network for session-based
recommendation. InIJCAI, volume 19, pages 3940â€“3946, 2019.
[36] Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Jiajie Xu, Victor S Sheng S. Sheng, Zhiming
Cui, Xiaofang Zhou, and Hui Xiong. Recurrent convolutional neural network for sequential
recommendation. InTheWebConf (former WWW), pages 3398â€“3404, 2019.
[37] An Yan, Shuo Cheng, Wang-Cheng Kang, Mengting Wan, and Julian McAuley. Cosrec: 2d
convolutional neural networks for sequential recommendation. InProceedings of the 28th ACM
international conference on information and knowledge management, pages 2173â€“2176, 2019.
[38] Jiyuan Yang, Yuanzi Li, Jingyu Zhao, Hanbing Wang, Muyang Ma, Jun Ma, Zhaochun Ren,
Mengqi Zhang, Xin Xin, Zhumin Chen, et al. Uncovering selective state space modelâ€™s
capabilities in lifelong sequential recommendation.arXiv preprint arXiv:2403.16371, 2024.
[39] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure
Leskovec. Graph convolutional neural networks for web-scale recommender systems. InKDD,
2018.
[40] Fajie Yuan, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M Jose, and Xiangnan He. A
simple convolutional generative network for next item recommendation. InProceedings of the
twelfth ACM international conference on web search and data mining, pages 582â€“590, 2019.
[41] Zhenrui Yue, Yueqi Wang, Zhankui He, Huimin Zeng, Julian McAuley, and Dong Wang. Linear
recurrent units for sequential recommendation. InProceedings of the 17th ACM International
Conference on Web Search and Data Mining, pages 930â€“938, 2024.
[42] Shun Zhang, Runsen Zhang, and Zhirong Yang. Matrrec: Uniting mamba and transformer for
sequential recommendation.arXiv preprint arXiv:2407.19239, 2024.
[43] Yixin Zhang, Yong Liu, Yonghui Xu, Hao Xiong, Chenyi Lei, Wei He, Lizhen Cui, and Chunyan
Miao. Enhancing sequential recommendation with graph contrastive learning.arXiv preprint
arXiv:2205.14837, 2022.
[44] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan
Wang, and Ji-Rong Wen. S3-rec: Self-supervised learning for sequential recommendation with
mutual information maximization. InCIKM, pages 1893â€“1902, 2020.
[45] Kun Zhou, Hui Yu, Wayne Xin Zhao, and Ji-Rong Wen. Filter-enhanced mlp is all you need for
sequential recommendation. InTheWebConf (former WWW), pages 2388â€“2399, 2022.
[46] Peilin Zhou, Qichen Ye, Yueqi Xie, Jingqi Gao, Shoujin Wang, Jae Boum Kim, Chenyu You,
and Sunghun Kim. Attention calibration for transformer-based sequential recommendation. In
CIKM, pages 3595â€“3605, 2023.
13
Supplementary Materials for â€œTV-Rec: Time-Variant Convolutional
Filter for Sequential Recommendationâ€
A Proof of Frequency Response of Node-Variant Graph Filters
In this section, we provide a detailed proof of the frequency response of the node-variant graph filter
Gnvas stated in Eq. (5), following the derivation in [6].
Proof. We begin by considering the definition of the node-variant graph filter. Let xâˆˆRNrepresent
the input graph signal, SâˆˆRNÃ—Nrepresent the shift operator, and let hkbe a vector of filter taps.
By using the node-variant graph filterG nv, the output graph signalyis calculated as follows:
y=G nvx=KX
k=0diag(h k)Sk
x,(14)
where diag(Â·) indicates constructing a diagonal matrix from a vector, and Kis the order of the filter.
Recall that the frequency response of the input signal ËœxisUâŠ¤x, and similarly Ëœy=UâŠ¤y, where the
shift operatorSis eigen-decomposed byS=Udiag(Î»)UâŠ¤, which results in:
Ëœy=UâŠ¤y=UâŠ¤KX
k=0diag(h k)Sk
x(15)
=UâŠ¤KX
k=0diag(h k)Udiag(Î»k)Ëœx,(16)
where Î»kâˆˆCNis the k-th column of a Vandermonde matrix Î›âˆˆCNÃ—(K+1)given by Î›ik=
[Î»k]i=Î»k
i. The set of filter taps hkcan be represented in matrix form as HâˆˆCNÃ—(K+1). Each
element of the summation in Eq. (16) is as follows:
"KX
k=0diag(h k)Udiag(Î»k)#
ij=KX
k=0h(i)
kÎ»k
juij (17)
=uijKX
k=0h(i)
kÎ»k
j,(18)
where h(i)
kis the i-th row and k-th column of H. Using the matrices HandÎ›, it holds that HÎ›âŠ¤âˆˆ
CNÃ—N:
[HÎ›âŠ¤]ij=KX
k=0h(i)
kÎ»k
j.(19)
Substituting Eq. (19) into Eq. (18) results in:
"KX
k=0diag(h k)Udiag(Î»k)#
ij= [Uâ—¦HÎ›âŠ¤]ij,(20)
whereâ—¦denotes element-wise multiplication between matrices.
Therefore, Eq. (16) becomes:
Ëœy= ËœGnvËœx=UâŠ¤(Uâ—¦(HÎ›âŠ¤))Ëœx.(21)
This completes the proof.
14
B Theoretical Justification for DCG-Based Filtering
This section theoretically shows that spectral filtering on the zero-padded DCG is equivalent to
filtering on the line graph, ensuring no backward information flow.
To perform spectral filtering, the model must apply GFT, which requires eigen decomposition of
the graph shift operator (i.e., the adjacency or Laplacian matrix). However, when the sequence is
modeled as a line graph, the resulting adjacency matrix has rank exactly Nâˆ’1 because the first node
(i.e., the most past item) has no incoming edges. This results in one row of the matrix being entirely
zero, making it defective and thus non-diagonalizable.
To resolve this, we model the sequence as a DCG, which differs from the line graph by a single edge
connecting the last node to the first. This addition makes the adjacency matrix circulant, ensuring
diagonalizability and enabling spectral filtering in the Fourier domain. However, the added edge
introduces a backward connection from the future to the past, which could lead to information leakage
in the reverse temporal direction.
To prevent reverse information flow while maintaining spectral tractability, we adopt a padding
strategy. Specifically, we pad the sequencexâˆˆRNwithKzeros by forming the extended vector:
Ëœx=
x
0
âˆˆRN+K,(22)
where 0âˆˆRKis the zero vector. Using the circulant shift operator SâˆˆR(N+K)Ã—(N+K)that
represents the DCG, we define a spectral filter of orderKas follows:
g(S) =KX
k=0hkSk,(23)
where hkdenotes the filter coefficients. The filtered output Ëœyis then computed by applying the filter
to the padded input:
Ëœy=g(S) Ëœx.(24)
We obtain a result identical to applying the same filter on the original sequence xmodeled as a line
graph by extracting the firstNelements of Ëœyas follows:
y= [ Ëœy]1:N,(25)
where 1 :N denotes taking the first Nelements of the vector. This equivalence holds because the
lastKentries of Ëœxare zeros. Although the circulant matrix Sperforms circular shifts, the filter
involves powers of Sup to order K, so these zero entries do not influence the first Noutput values.
Consequently, this effectively blocks any cyclic information flow that would lead to backward leakage.
As a result, spectral filtering on the zero-padded DCG provides the correct filtering output equivalent
to causal convolution on a line graph, while benefiting from the computational efficiency and
diagonalizability of circulant matrices.
C Equivalence of Positional Encoding and Graph Fourier Basis
Unlike Transformers, which require explicit positional encoding (e.g., sinusoidal or learnable), TV-
Rec captures positional information inherently through spectral decomposition on DCG. This section
provides a formal connection between positional encodings in Transformer and the graph Fourier
basis used in TV-Rec.
Sinusoidal Positional Encoding in Transformer.The original Transformer model uses sinusoidal
functions to encode absolute positionposas follows:
PE(pos,2i) = sinpos
100002i/d
,PE (pos,2i+1) = cospos
100002i/d
,
where ddenotes the embedding dimension. This produces a set of periodic signals with varying
frequencies that form a basis for encoding positional variation.
15
Graph Fourier Basis in TV-Rec.In TV-Rec, we define the shift operator Sas the adjacency matrix
of a directed cyclic graph. Its eigen-decomposition yields the graph Fourier basis UâˆˆCNÃ—N, where:
Ukn=1âˆš
Neâˆ’i2Ï€kn/N=1âˆš
N
cos2Ï€kn
N
âˆ’isin2Ï€kn
N
.
This corresponds to the DFT basis, which is an orthonormal set of complex exponential spanning RN
(orCN).
Equivalence in Representational Capacity.While the frequency components used in Transformer
positional encodings are sampled on a logarithmic scale and those in GFT are linearly spaced, both
sets of basis functions are composed of trigonometric functions. As such, they span the same space
of length- Nperiodic signals. Formally, the real-valued DFT basis used in TV-Rec corresponds to
sin 2Ï€kn
N
andcos 2Ï€kn
N
for various k, which can represent any finite-length sinusoidal signal,
including those used in Transformer encodings. Therefore, although the sampling strategies differ,
the span of both bases covers the same function space, which indicates that the spectral basis used in
TV-Rec is functionally equivalent to the sinusoidal encodings in Transformer models, in the sense
that both span the same space of position-dependent trigonometric functions.
Implication for Positional Encoding.TV-Rec projects the input sequence xto the GFT domain
asËœx=UâŠ¤x, and reconstructs it via x=UËœx , thereby implicitly encoding frequency-based posi-
tional variations. The time-variant filter then modulates these frequency components per position,
which makes explicit positional embeddings unnecessary, as position-sensitive modulation is already
embedded in the spectral filtering process. TV-Recâ€™s design leverages the spectral basis to achieve
the same functional role as positional encoding in Transformers. It achieves this without requiring
explicit embeddings, while retaining full expressiveness in modeling temporal variation.
D Detailed Experimental Settings
D.1 Datasets
We evaluate our model using 6 SR datasets that vary in sparsity and domain. We follow the data pre-
processing procedures outlined in [ 44,45], considering all reviews and ratings as implicit feedback.
Detailed statistics can be found in Table 6.
â€¢Amazon BeautyandSportsare Amazon datasets of product reviews from [ 22], widely
used for SR. For this study, we use the â€œBeautyâ€ and â€œSports and Outdoorsâ€ categories.
â€¢Yelp4is a popular business recommendation dataset. We use records from after 2019/01/01
due to its large size.
â€¢LastFM5includes artist listening records and is used to recommend musicians to users.
â€¢ML-1M[ 11] is a movie recommendation dataset from MovieLens6. It is commonly used to
evaluate recommendation algorithms due to its detailed user interaction data.
â€¢Foursquare7provides user check-ins across New York city over 10 months (April 2012 to
February 2013).
D.2 Baselines
To evaluate the performance of our method, we compare it with the following ten SR baseline
methods:
â€¢Caser[ 30] is a CNN-based model that captures complex user patterns through horizontal
and vertical convolutions.
4https://www.yelp.com/dataset
5https://grouplens.org/datasets/hetrec-2011/
6https://grouplens.org/datasets/movielens/
7https://sites.google.com/site/yangdingqi/home/foursquare-dataset
16
Table 6: Statistics of the processed datasets.
# Users # Items # Interactions Avg. Length Sparsity
Beauty 22,363 12,101 198,502 8.9 99.93%
Sports 25,598 18,357 296,337 8.3 99.95%
Yelp 30,431 20,033 316,354 10.4 99.95%
LastFM 1,090 3,646 52,551 48.2 98.68%
ML-1M 6,041 3,417 999,611 165.5 95.16%
Foursquare 1,083 9,989 179,468 165.7 98.34%
â€¢GRU4Rec[ 13] is a GRU-based model that captures temporal dynamics and patterns in user
interactions.
â€¢SASRec[ 17] is a Transformer-based model that uses a multi-head self-attention mechanism.
â€¢BERT4Rec[ 29] is a bidirectional Transformer-based model, using a masked item training
scheme.
â€¢NextItNet[ 40] is a CNN-based model that uses dilated convolutions and residual connec-
tions to capture both short- and long-range dependencies in user behavior sequences.
â€¢FMLPRec[ 45] uses Fourier Transform and learnable filters in an all-MLP architecture to
reduce noise and enhance sequence representations.
â€¢DuoRec[ 23] employs model-level augmentation and sementic positive samples for con-
trastive learning, using SASRec as its base model.
â€¢LRURec[41] uses linear recurrent units for rapid inference and recursive parallelization.
â€¢AdaMCT[ 16] is a hybrid model combining Transformer attention with local convolutional
filters to capture long- and short-term user preferences.
â€¢BSARec[ 28] is a hybrid model that combines Transformer self-attention with the Fourier
Transform to address the oversmoothing problem.
â€¢SR-GNN[ 33] is a session-based recommendation model which converts user sessions into
graphs and applies GNNs to capture item transition relationships.
â€¢GC-SAN[ 35] is a GNN-based model that dynamically builds a graph for each sequence and
combines GNN with self-attention to model both local and long-range item dependencies.
â€¢GCL4SR[ 43] is a GNN-based model that constructs a global item transition graph across
all users and uses graph contrastive learning to integrate global and local context.
D.3 Metrics
To evaluate the recommendation performance, we use widely adopted Top- rmetrics, HR@ r(Hit
Rate) and NDCG@ r(Normalized Discounted Cumulative Gain), with rset to 5, 10, and 20. For a fair
comparison, we examine the ranking results across the entire item set without negative sampling [ 19].
D.4 Implementation Details
All experiments, including the baselines, were conducted using the following software and hard-
ware configurations: UBUNTU20.04.6 LTS, PYTHON3.9.7, PYTORCH2.2.2, CUDA 11.1.74, and
NVIDIA Driver 550.54.14. The hardware setup included dual INTELXEONCPUs and an NVIDIA
RTX A6000 GPU.
Hyperparameters for Standard Sequential Recommendation.We determine the optimal hyper-
parameters for the baselines according to their suggested settings. The experiments are performed
with the following hyperparameters: learning rates of { 5Ã—10âˆ’4,1Ã—10âˆ’3}, orthogonal regularization
coefficient Î±of {0,1Ã—10âˆ’3,1Ã—10âˆ’5}, dropout rates pof {0.1, 0.2, 0.3, 0.4, 0.5}, and mvalues of
{8, 16, 32}. The order of the time-variant convolutional filter Kis equal to the maximum sequence
length N, which is set to 50. The batch size is set to 256, the dimension Dto 64, and the number
of time-variant blocks Lto 2. We use the Adam optimizer for training. For reproducibility, the best
hyperparameter settings are detailed in Table 7.
17
Table 7: Best hyperparameters of TV-Rec.
Dataset Learning RateÎ± p m
L=50
Beauty5Ã—10âˆ’40 0.5 32
Sports5Ã—10âˆ’40 0.5 16
Yelp5Ã—10âˆ’41Ã—10âˆ’30.1 16
LastFM1Ã—10âˆ’31Ã—10âˆ’30.4 8
ML-1M1Ã—10âˆ’31Ã—10âˆ’50.3 8
Foursquare5Ã—10âˆ’41Ã—10âˆ’50.2 8
L=200
ML-1M1Ã—10âˆ’30 0.1 16
Foursquare5Ã—10âˆ’41Ã—10âˆ’50.1 8
48163250
m0.066 0.068 0.070NDCG@20NDCG@20
HR@20
0.1360.1380.140
HR@20
(a) Beauty
48163250
m0.041 0.0420.042NDCG@20NDCG@20
HR@20
0.0850.086 0.088
HR@20 (b) Sports
48163250
m0.0300.0310.032NDCG@20NDCG@20
HR@20
0.073 0.075 0.078
HR@20 (c) Yelp
48163250
m0.0460.051 0.058NDCG@20NDCG@20
HR@20
0.104 0.112 0.120
HR@20
(d) LastFM
48163250
m0.190 0.193 0.196NDCG@20NDCG@20
HR@20
0.392 0.400 0.408
HR@20 (e) ML-1M
48163250
m0.007 0.012 0.017NDCG@20NDCG@20
HR@20
0.0180.024 0.031
HR@20 (f) Foursquare
Figure 8: Sensitivity to the number of basis vectorsm.
E Sensitivity Studies
In this section, we investigate the sensitivity of four hyperparameters: the number of basis vector m,
dropout rate p, orthogonal regularization coefficient Î±and filter order K. The results are shown in
Figs. 8, 9, 10 and Table 8 respectively. We keep optimal settings for all other hyperparameters except
the one being examined.
Sensitivity to m.In our time-variant graph filter, we use a basis matrix BâˆˆRmÃ—(K+1)as shown
in Eq. (8). The parameter mdetermines the number of basis vectors, which affects the expressive
power and computational complexity. To explore its effect, we extend the initial parameter search
range of m={8,16,32} to include m={4,50} . Fig. 8 shows NDCG@20 and HR@20 by varying
m. For Beauty, the best accuracy is achieved with m= 32 , and both NDCG@20 and HR@20
drop significantly as mdecreases. This suggests that for Beauty, a larger number of basis vectors
is beneficial, possibly due to the complexity of user-item interactions in this dataset. In contrast,
for LastFM, the best accuracy is achieved with m= 8 , and as mincreases, both NDCG@20 and
HR@20 decline dramatically. This behavior may be attributed to the particular characteristics of
music recommendation on LastFM, where a more condensed representation is sufficient to capture
user preferences.
18
0.10.20.30.40.5
p0.067 0.069 0.071NDCG@20NDCG@20
HR@20
0.1330.136 0.140
HR@20(a) Beauty
0.10.20.30.40.5
p0.0370.039 0.042NDCG@20NDCG@20
HR@20
0.0790.083 0.088
HR@20 (b) Sports
0.10.20.30.40.5
p0.029 0.0310.032NDCG@20NDCG@20
HR@20
0.0730.075 0.078
HR@20 (c) Yelp
0.10.20.30.40.5
p0.0420.049 0.057NDCG@20NDCG@20
HR@20
0.0940.106 0.120
HR@20
(d) LastFM
0.10.20.30.40.5
p0.175 0.185 0.196NDCG@20NDCG@20
HR@20
0.373 0.390 0.408
HR@20 (e) ML-1M
0.10.20.30.40.5
p0.0110.0140.017NDCG@20NDCG@20
HR@20
0.0260.028 0.031
HR@20 (f) Foursquare
Figure 9: Sensitivity to dropout ratep.
01e-71e-51e-31e-1
Â®0.0700.070NDCG@20NDCG@20
HR@20
0.1380.1390.140
HR@20
(a) Beauty
01e-71e-51e-31e-1
Â®0.0410.041 0.042NDCG@20NDCG@20
HR@20
0.0850.086 0.088
HR@20 (b) Sports
01e-71e-51e-31e-1
Â®0.0310.031 0.032NDCG@20NDCG@20
HR@20
0.0770.077 0.078
HR@20 (c) Yelp
01e-71e-51e-31e-1
Â®0.053 0.055 0.057NDCG@20NDCG@20
HR@20
0.108 0.114 0.120
HR@20
(d) LastFM
01e-71e-51e-31e-1
Â®0.187 0.191 0.195NDCG@20NDCG@20
HR@20
0.3950.401 0.408
HR@20 (e) ML-1M
01e-71e-51e-31e-1
Â®0.0130.0150.017NDCG@20NDCG@20
HR@20
0.0300.031 0.033
HR@20 (f) Foursquare
Figure 10: Sensitivity to orthogonal regularization coefficientÎ±.
Sensitivity to p.The effect of the dropout rate pis analyzed in Fig. 9. For Sports, a larger value
ofpleads to better performance. Conversely, for Foursquare, a lower value of presults in improved
performance. Datasets with many interactions tend to achieve better accuracy with a small p, whereas
those with fewer interactions typically perform better with a large p. This is because lower data
diversity of datasets with fewer interactions leads model to easily overfit. A higher dropout rate helps
prevent overfitting by encouraging the model to learn more general patterns.
Sensitivity to Î±.The parameter search range for Î±is extended from { 0,1Ã—10âˆ’3,1Ã—10âˆ’5} to
{0,1Ã—10âˆ’1,1Ã—10âˆ’3,1Ã—10âˆ’5,1Ã—10âˆ’7}, and experiments are conducted with these values.
Fig. 10 shows NDCG@20 and HR@20 by varying Î±. For LastFM, the best accuracy is achieved
19
Table 8: Sensitivity to filter orderK.
KBeauty Sports Yelp LastFM ML-1M Foursquare
H@20 N@20 H@20 N@20 H@20 N@20 H@20 N@20 H@20 N@20 H@20 N@20
30.1380 0.0690 0.0839 0.0405 0.0731 0.0295 0.1046 0.0443 0.3901 0.1843 0.0240 0.0116
50.1364 0.0680 0.0849 0.0407 0.0752 0.0308 0.1147 0.0490 0.3823 0.1814 0.0268 0.0116
100.1378 0.0688 0.0853 0.0410 0.0741 0.0303 0.1165 0.0532 0.3922 0.1898 0.0268 0.0139
250.1371 0.0684 0.0854 0.0409 0.0774 0.0323 0.12480.0546 0.3957 0.1891 0.0295 0.0163
500.1403 0.0705 0.0880 0.0425 0.07770.0321 0.1202 0.0572 0.4079 0.1955 0.0314 0.0176
Table 9: Results on XLong.
Metric SASRec LRURec TV-Rec Improv.
HR@5 0.3612 0.4266 0.484413.55%
HR@10 0.4680 0.5137 0.53534.20%
HR@20 0.56120.5874 0.5774 -1.70%
NDCG@5 0.2656 0.3227 0.390521.00%
NDCG@10 0.2979 0.3510 0.407115.98%
NDCG@20 0.3232 0.3697 0.417813.01%
withÎ±= 10âˆ’3. Both NDCG@20 and HR@20 drop significantly as Î±decreases, suggesting that an
adequate level of orthogonal regularization is crucial to maintain effective filter diversity and prevent
overfitting in this dataset. In contrast, for Sports and Yelp datasets, the highest accuracy occurs at
relatively lower Î±values, and performance deteriorates noticeably as Î±increases. This behavior may
indicate that too strong orthogonal regularization overly constrains the filter parameters, limiting
the modelâ€™s ability to adapt to the data distribution in these domains. These results emphasize the
importance of adjusting Î±based on the characteristics of the dataset, balancing the regularization
strength to achieve optimal performance.
Sensitivity to filter orderK.We analyze the sensitivity of the filter orderK, which is equivalent
to the kernel size in CNN-based models. As shown in Table 8, except for Yelp and LastFM, setting
K= 50 consistently yields the best performance across datasets. This supports our hypothesis that
aligning the shift depth Kwith the sequence length Nenables the model to capture more global
context, thereby enhancing recommendation quality.
F Additional Results on XLong
To further test scalability in extreme cases, we conducted experiments on the XLong dataset, which
contains 69,069 users, 2.12 million items, and approximately 66.8 million interactions. The sequences
are exceptionally long, with an average length of 958.8 and a density of about 5Ã—10âˆ’4, roughly 20
times longer than in our main experiments. We followed the experimental settings of LRURec [ 41]
and performed the experiments within the LRURec framework. As shown in Table 9, although TVRec
shows slightly lower performance compared to LRURec in Recall@20, it significantly outperforms
in all other metrics. This demonstrates that TVRec handles extremely long sequences effectively,
highlighting its strength in long-range modeling tasks.
G Model Complexity and Runtime Analyses
In this section, we provide a comprehensive analysis of model complexity and runtime efficiency
for the entire datasets. Table 10 shows a detailed comparison of our proposed TV-Rec model with
7 baseline models in 6 different datasets. Our TV-Rec consistently shows competitive parameter
efficiency in all datasets, maintaining a comparable or slightly lower number of parameters than most
advanced baselines. In terms of training efficiency, our TV-Rec shows competitive training cost in
most datasets, often comparable to or slightly higher than SASRec and FMLPRec. TV-Rec shows
strong inference efficiency, often having the lowest or near-lowest inference costs among all models,
especially in datasets such as Beauty, Sports, and Yelp.
20
Table 10: Parameters number and execution efficiency analysis of models.
Dataset Metrics TV-Rec AdaMCT BSARec FMLPRec NextItNet BERT4Rec SASRec Caser
Beauty# Parameters 854,208 878,208 880,318 851,200 981,696 877,888 877,824 2,909,532
Training Cost (s/epoch) 13.20 12.30 11.35 11.85 18.9184 21.98 11.21 65.54
Inference Cost (s/epoch) 0.5697 0.6647 0.7299 0.5427 1.0267 0.5821 0.6316 1.0641
NDCG@20 0.0704 0.0691 0.0664 0.0419 0.0547 0.0484 0.0379 0.0164
Sports# Parameters 1,248,160 1,278,592 1,264,318 1,251,584 1,644,224 1,278,272 1,278,208 4,835,756
Training Cost (s/epoch) 19.46 17.63 18.44 17.76 30.1034 31.60 14.26 98.21
Inference Cost (s/epoch) 0.7411 0.9049 0.9679 0.7049 1.2724 0.8016 0.8460 1.5715
NDCG@20 0.0428 0.0422 0.0379 0.0239 0.0316 0.0288 0.0217 0.0109
Yelp# Parameters 1,355,424 1,385,856 1,365,522 1,358,848 1,751,488 1,385,536 1,385,472 3,925,238
Training Cost (s/epoch) 21.89 20.87 21.83 20.20 32.88 37.57 16.74 111.62
Inference Cost (s/epoch) 0.6388 0.8527 0.8890 0.6223 1.2348 0.7225 0.7322 1.3220
NDCG@20 0.0338 0.0290 0.0272 0.0211 0.0276 0.0275 0.0180 0.0151
LastFM# Parameters 303,440 337,088 322,814 310,080 981,696 336,768 336,704 998,646
Training Cost (s/epoch) 2.41 2.51 2.66 2.51 19.1253 4.29 2.30 13.22
Inference Cost (s/epoch) 0.2649 0.2807 0.3228 0.2678 1.068 0.3018 0.3061 0.3445
NDCG@20 0.0582 0.0514 0.0485 0.0475 0.0402 0.0366 0.0458 0.0306
ML-1M# Parameters 298,368 322,368 308,094 295,360 556,928 322,048 321,984 961,326
Training Cost (s/epoch) 26.60 27.68 31.40 24.67 40.7731 46.51 22.50 102.99
Inference Cost (s/epoch) 0.4129 0.4180 0.4474 0.3594 0.7269 0.4202 0.4181 0.5817
NDCG@20 0.1951 0.1836 0.1711 0.1388 0.1829 0.1588 0.1438 0.1101
Foursquare# Parameters 719,040 743,040 728,766 716,032 1,108,672 742,720 742,656 1,073,044
Training Cost (s/epoch) 6.01 5.79 5.81 5.11 8.9109 9.06 5.27 21.01
Inference Cost (s/epoch) 0.2900 0.2913 0.3562 0.2521 0.4963 0.2871 0.3178 0.3703
NDCG@20 0.0170 0.0147 0.0131 0.0110 0.0115 0.0132 0.0137 0.0133
Table 11: Performance comparison between TV-Rec and the second-best baseline methods across 6
datasets. Results show the mean and standard deviation for 10 runs with different random seeds using
the best hyperparameter settings.
Datasets Methods HR@5 HR@10 HR@20 NDCG@5 NDCG@10 NDCG@20
BeautyBSARec 0.0694Â±0.0010.0978Â±0.0020.1352Â±0.0020.0496Â±0.0010.0587Â±0.0010.0681Â±0.001
TV-Rec 0.0706Â±0.0010.0997Â±0.0010.1375Â±0.0020.0500Â±0.0010.0594Â±0.0010.0689Â±0.001
SportsBSARec 0.0417Â±0.0010.0600Â±0.0010.0844Â±0.0010.0288Â±0.0010.0349Â±0.0010.0411Â±0.001
TV-Rec 0.0420Â±0.0010.0610Â±0.0020.0863Â±0.0020.0290Â±0.0010.0351Â±0.0010.0415Â±0.001
YelpDuoRec 0.0268Â±0.0010.0453Â±0.0010.0733Â±0.0010.0170Â±0.0000.0230Â±0.0000.0300Â±0.000
TV-Rec 0.0284Â±0.0010.0472Â±0.0010.0759Â±0.0010.0179Â±0.0000.0240Â±0.0010.0312Â±0.001
LastFMBSARec 0.0501Â±0.0040.0707Â±0.0060.1051Â±0.0080.0342Â±0.0020.0412Â±0.0020.0498Â±0.002
TV-Rec 0.0508Â±0.0050.0750Â±0.0060.1090Â±0.0070.0343Â±0.0030.0420Â±0.0030.0506Â±0.003
ML-1MLRURec 0.1955Â±0.0020.2818Â±0.0020.3871Â±0.0020.1326Â±0.0020.1604Â±0.0020.1869Â±0.002
TV-Rec 0.2024Â±0.0050.2901Â±0.0050.3972Â±0.0050.1365Â±0.0040.1647Â±0.0030.1918Â±0.003
FoursquareBSARec 0.0133Â±0.0030.0175Â±0.0030.0250Â±0.0030.0098Â±0.0020.0111Â±0.0020.0130Â±0.002
TV-Rec 0.0151Â±0.0020.0214Â±0.0030.0289Â±0.0020.0105Â±0.0020.0126Â±0.0020.0145Â±0.002
Overall, TV-Rec presents a balance between model complexity, computational efficiency, and rec-
ommendation performance. Our TV-Rec achieves the best performance metrics while maintaining
competitive or better efficiency in both training and inference compared to state-of-the-art models.
H Statistical Significance of Experimental Results
To ensure the reliability of our evaluation, we conducted each experiment using 10 different random
seeds under the best hyperparameter settings for both our proposed model and the second-best
baseline model. The second-best models for each dataset are as follows: BSARec for Beauty, Sports,
LastFM, and Foursquare; DuoRec for Yelp; and LRURec for ML-1M. We then report the mean and
standard deviation of the performance metrics calculated across these runs to reflect variability caused
by random initialization. The detailed results, including these statistics, are provided in Table 11.
21
I Broader Impact
This work proposes TV-Rec, a time-variant convolutional filter for sequential recommendation. Its
improved efficiency and performance can positively impact real-world recommender systems by
reducing computational cost and energy consumption. However, potential negative societal impacts
include reinforcing user biases, limiting content diversity, and risking privacy violations if sensitive
user behavior data is mishandled. TV-Recâ€™s reliance on user interaction logs raises concerns about
data privacy and fairness in recommendations. To mitigate these issues, we recommend careful data
governance, fairness auditing, and the development of privacy-preserving training pipelines.
J Limitation
While TV-Rec achieves strong performance and inference-time efficiency, it has several limitations.
First, the training process of TV-Rec involves some additional computational complexity due to
repeated GFT and inverse GFT operations, as well as the generation of position-specific filters.
Moreover, to enable spectral filtering, TV-Rec replaces the original line graph with DCG, which adds
extra nodes through padding and increases the dimensionality of the spectral domain. These factors
can lead to moderate increases in training time and memory usage. However, as shown in Table 10,
this overhead remains manageable and does not significantly impact training scalability in practice.
Second, the filter generation in TV-Rec is data-independent, relying solely on temporal position
rather than sequence content. While this design improves generalization and structural simplicity, it
may limit the modelâ€™s adaptability to instance-specific behavior or irregular patterns. Nonetheless,
the inference-time efficiency and architectural simplicity make these trade-offs acceptable for many
practical applications. Overall, these trade-offs are justified by TV-Recâ€™s strong empirical results and
practical efficiency.
22